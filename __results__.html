<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Vicens Gaitan" />


<title>Image registration, the R way, (almost) from scratch</title>

<script src="__results___files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="__results___files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="__results___files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="__results___files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="__results___files/bootstrap-3.3.5/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="__results___files/highlight/default.css"
      type="text/css" />
<script src="__results___files/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="__results___files/navigation-1.0/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Image registration, the R way, (almost) from scratch</h1>
<h4 class="author"><em>Vicens Gaitan</em></h4>
<h4 class="date"><em>15 de mayo de 2016</em></h4>

</div>


<div id="motivation" class="section level2">
<h2>Motivation</h2>
<p>As far I know, there is no package for image registration in R and no wrapper to open.CV, so I’ll try to write some simple functions to detect keypoints in images, calculate descriptors , find matches and adjust a homomorphic transformation between images.</p>
<p>There are some packages in R for image manipulation and after some test I select “imager” , based on the CImg C++, fast and providing several image processing tools.</p>
</div>
<div id="reading-and-plotting-images" class="section level2">
<h2>Reading and plotting images</h2>
<p>Let’s read an image using the “jpeg” package, and transform to a CImg object for manipulation with “imager”</p>
<pre class="r"><code>set=10;id=4
nim=paste0(path,&quot;set&quot;,set,&quot;_&quot;,id,&quot;.jpeg&quot;)
im=readJPEG(nim)
str(im)</code></pre>
<pre><code>##  num [1:2329, 1:3100, 1:3] 0.729 0.725 0.725 0.718 0.714 ...</code></pre>
<p>the im object is an 2-d numerical array with depth 3 (for the RGB channels) We can work with the full color image or with a channel. The plot command is able to display a CImg object</p>
<pre class="r"><code>cim=imresize(as.cimg(im),scale = scl)</code></pre>
<pre><code>## Warning in as.cimg.array(im): Assuming third dimension corresponds to
## colour</code></pre>
<pre class="r"><code>str(cim)</code></pre>
<pre><code>##  cimg [1:698, 1:930, 1, 1:3] 0.722 0.712 0.702 0.679 0.71 ...</code></pre>
<pre class="r"><code>cimr=imresize(as.cimg(im[,,1]),scale=scl)#Red Channel
str(cimr)</code></pre>
<pre><code>##  cimg [1:698, 1:930, 1, 1] 0.722 0.712 0.702 0.679 0.71 ...</code></pre>
<pre class="r"><code>par(mfrow=c(1,2))
plot(cim)
plot(cimr)</code></pre>
<p><img src="__results___files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="keypoint-detection" class="section level2">
<h2>Keypoint detection</h2>
<p>The first step in the registration process consist in the keypoint detection. The points selected must be robust to translations, rotations and scaling and stables to noise. There is a handful of methodologies, some of them subject to patents like SIFT, but simpler ideas can also work. In this case we can try with a “corner detector”. The Harris detector is based in gradients and gaussian filtering, and is easily implemented with imager. The value of the Harris transformation is high only in well defined corners on the image. We select as keypoints the center of connected regions with Harris over a certain threshod.</p>
<pre class="r"><code>#HARRIS - Harris corner detector
Harris&lt;-function(im,sigma=2){
  eps=1.e-10
  ix=imgradient(im,&quot;x&quot;)
  iy=imgradient(im,&quot;y&quot;)
  ix2=isoblur(ix*ix,sigma,gaussian = T)
  iy2=isoblur(iy*iy,sigma,gaussian = T)
  ixy=isoblur(ix*iy,sigma,gaussian = T)
  (ix2*iy2-ixy*ixy)/(ix2+iy2+eps)
}
cim_Harris=Harris(cimr,sigma=3*scl)


#Detect Keypoints
get.centers &lt;- function(im,thr=&quot;99%&quot;,sigma=3*scl,bord=30*scl){
  dt &lt;- Harris(im,sigma) %&gt;% imager::threshold(thr) %&gt;% label
  as.data.frame(dt) %&gt;% subset(value&gt;0 ) %&gt;% dplyr::group_by(value) %&gt;% dplyr::summarise(mx=round(mean(x)),my=round(mean(y))) %&gt;% subset(mx&gt;bord &amp; mx&lt;width(im)-bord &amp; my&gt;bord &amp; my&lt;height(im)-bord)
}
par(mfrow=c(1,2))
plot(log(cim_Harris+.0001))
plot(cimr)
# Detect keypoints 
kp=as.data.frame(cimr %&gt;% get.centers(sigma=3*scl,&quot;98%&quot;))[,2:3]
kp %$% points(mx,my,col=&quot;red&quot;)</code></pre>
<p><img src="__results___files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>head(kp)</code></pre>
<pre><code>##    mx my
## 1 474 69
## 2 509 17
## 3 404 16
## 4 262 10
## 5 328 10
## 6 476 11</code></pre>
</div>
<div id="image-orientation" class="section level2">
<h2>Image Orientation</h2>
<p>The next step is to build point descriptors. A simple solution is to use a normalized patch around a blured region of each key point. For instance a grid of 9x9 pixels taken from a 20x20 patch, every 5 pixels</p>
<pre class="r"><code>stencil &lt;- expand.grid(dx=seq(-20,20,5)*scl,dy=seq(-20,20,5)*scl)</code></pre>
<p>Usually the patch must be oriented using the main patch direction, but in this case it seems to work better to use a global orientation for all patches. To do that we must calculate a global direction for the whole image. This is done using the histogram of gradient (HoG) orientations and selecting the maximum (correcting for +/- 90º rotations)</p>
<pre class="r"><code>sigma_b=6*scl
ima_bl=isoblur(cimr,sigma_b,gaussian = T)

pi=3.141592653

ix=imgradient(ima_bl,&quot;x&quot;)
iy=imgradient(ima_bl,&quot;y&quot;)
ita=atan(iy/ix)*180/pi
hist(ita,breaks=180)</code></pre>
<p><img src="__results___files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>The peak in the histogram of gradients define the main image direction. Some images can have more than one local maxima in the Hog, defining several possible image orientation Let’s define a function to calculate it.</p>
<pre class="r"><code>#Return 1 or 2 main global image orientations restricted to (-45,45)

get_orientations&lt;-function(im){
  ix=imgradient(im,&quot;x&quot;)
  iy=imgradient(im,&quot;y&quot;)
  ita=atan(iy/ix)*180/pi
  iga=table(sample(round(ita*2)/2,200000))
  #plot(iga)
  ma1=max(iga)[1]
  m1=which(iga==ma1)
  theta_1=(as.numeric(names(m1)))
  iga[max((m1-20),0):min((m1+20),length(iga))]=0
  #plot(iga)
  ma2=max(iga)[1]
  m2=which(iga==ma2)
  theta_2=(as.numeric(names(m2)))
  if(theta_1&gt;45) theta_1=theta_1-90
  if(theta_1&lt;(-45))theta_1=theta_1+90
  if(theta_2&gt;45) theta_2=theta_2-90
  if(theta_2&lt;(-45))theta_2=theta_2+90
  if(abs(theta_1-theta_2)&gt;5){
    return(c(theta_1,theta_2))
  }
  else{
    return(theta_1)
  }
}

th=get_orientations(ima_bl)
print(paste0(&quot;Global main directions &quot;,th,&quot;º&quot;))</code></pre>
<pre><code>## [1] &quot;Global main directions -6.5º&quot; &quot;Global main directions 0.5º&quot;</code></pre>
<pre class="r"><code>par(mfrow=c(1,length(th)))
for(theta in th)plot(imrotate(cimr,-theta))</code></pre>
<p><img src="__results___files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="keypoint-descriptor" class="section level2">
<h2>Keypoint descriptor</h2>
<p>Now we can extract the descriptor from a rotated patch, asuring all path are equally oriented across images. If the image had more than one orientation, we have to calculate one patch for every possible orientation. Lets plot some patches</p>
<pre class="r"><code>stencil &lt;- expand.grid(dx=round(seq(-20,20,5)*scl),dy=round(seq(-20,20,5)*scl))
stencil_ext &lt;- expand.grid(dx=round(seq(-30*scl,30*scl,1)),dy=round(seq(-30*scl,30*scl,1)))
par(mfrow=c(3,3))


for(i in c(12,50,100)){ #3 random keypoints
  pm=get.stencil(cimr,stencil_ext,x=kp[i,1],y=kp[i,2])
  pm=as.cimg(pm)
  plot(pm)
  imr=imrotate(pm,-th[1])
  plot(imr)
  ww=round(width(imr)/2)
  desc=get.stencil(imr,stencil,x=ww,y=ww)
  plot(as.cimg(desc))
}</code></pre>
<p><img src="__results___files/figure-html/unnamed-chunk-7-1.png" width="960" /></p>
<pre class="r"><code>#Get oriented descriptors
get_descriptor_oriented&lt;-function(im,theta,v){
  pm=get.stencil(im,stencil_ext,x=v[,1],y=v[,2])
  w=sqrt(length(pm))
  pm=as.cimg(pm,x=w,y=w)
  imr=imrotate(pm,-theta)
  ww=round(width(imr)/2)
  get.stencil(imr,stencil,x=ww,y=ww)
}</code></pre>
</div>
<div id="matching-of-keypoints" class="section level2">
<h2>Matching of keypoints</h2>
<p>Let’s try now with a pair of images:</p>
<pre class="r"><code>ida=1;idb=4
nim=paste0(path,&quot;set&quot;,set,&quot;_&quot;,ida,&quot;.jpeg&quot;)
ima=imresize(as.cimg(readJPEG(nim)[,1:3099,1]),scale=scl)
nim=paste0(path,&quot;set&quot;,set,&quot;_&quot;,idb,&quot;.jpeg&quot;)
imb=imresize(as.cimg(readJPEG(nim)[,1:3099,1]),scale=scl)

sigma_b=6*scl

ima_bl=isoblur(ima,sigma_b,gaussian = T)
imb_bl=isoblur(imb,sigma_b,gaussian = T)


tha=get_orientations(ima_bl)
thb=get_orientations(imb_bl)

par(mfrow=c(1,2))

plot(imrotate(ima,-tha[1]))
plot(imrotate(imb,-thb[1]))</code></pre>
<p><img src="__results___files/figure-html/unnamed-chunk-8-1.png" width="960" /></p>
<pre class="r"><code>par(mfrow=c(1,2))

plot(ima)
# Detect keypoints 
kpa=as.data.frame(ima %&gt;% get.centers(sigma=3*scl,&quot;98%&quot;))[,2:3]
kpa %$% points(mx,my,col=&quot;red&quot;)

plot(imb)
kpb=as.data.frame(imb %&gt;% get.centers(sigma=3*scl,&quot;98%&quot;))[,2:3]
kpb %$% points(mx,my,col=&quot;red&quot;)</code></pre>
<p><img src="__results___files/figure-html/unnamed-chunk-8-2.png" width="960" /></p>
<pre class="r"><code>feata=NULL
for(theta in tha){
  dfa&lt;-alply(kpa,1,function(v){ ss=get_descriptor_oriented(ima_bl,theta,v)}) %&gt;% do.call(rbind,.)
  dfa=as.data.frame(t(apply(dfa,1,scale)))
  feata &lt;- rbind(feata,dfa)
}
featb=NULL
for(theta in thb){
  dfb&lt;- alply(kpb,1,function(v){ ss=get_descriptor_oriented(imb_bl,theta,v)})  %&gt;% do.call(rbind,.)
  dfb=as.data.frame(t(apply(dfb,1,scale)))
  featb &lt;- rbind(featb,dfb)
}</code></pre>
<p>Now feata and featb contains the descriptors for both images. The next step is to find approximate matching between them. We will use knn to find points close in the feature space: we’ll kept first neighbors when the second neighbor distance is bigger within a factor</p>
<pre class="r"><code>require(FNN)

kk&lt;-get.knnx(data=feata, query=featb, k=2, algorithm =&quot;kd_tree&quot; )
if(length(thb)==1){
  lpb=c(1:nrow(kpb))
}else{
  lpb=c(c(1:nrow(kpb),c(1:nrow(kpb))))
}
if(length(tha)==2)kpa=rbind(kpa,kpa)

mask=(kk$nn.dist[,1]/kk$nn.dist[,2]&lt;.8)
match=cbind(kk$nn.index[mask,1],lpb[mask])               

p1=as.matrix(kpa[match[,1],])
p2=as.matrix(kpb[match[,2],])

par(mfrow=c(1,1))
plot(kk$nn.dist[,1],kk$nn.dis[,2],pch=&#39;.&#39;)
points(kk$nn.dist[mask,1],kk$nn.dis[mask,2],pch=&#39;o&#39;,col=&quot;red&quot;)</code></pre>
<p><img src="__results___files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="ransac-random-sample-consensus" class="section level2">
<h2>RANSAC (Random Sample Consensus)</h2>
<p>Probably, most of the matchings are spurious, so we need a robust method to fit the affine transformation from one set of points to the other. With RANSAC, we select ramdomly sets of 4 points for adjusting an homography, and record the number of the remaining points that agree with this parameters within an error. Then we select as “inliers” the most populated set and fit a final homography only to them.</p>
<p>Some function definitions:</p>
<pre class="r"><code># Estimate a homography h from points in P to points in p
est_homograph&lt;-function(P,p){
  n=nrow(P)
  hh=NULL
  for(i in 1:n){
    a=t(c(p[i,],1))
    b=t(c(0,0,0))
    c=P[i,]
    d=-c%*%a
    hh=rbind(hh,cbind(rbind(c(a,b),c(b,a)),d))
  }
  h=t(matrix(svd(hh,nv=ncol(hh))$v[,9],nrow=3,ncol=3))
}

#Apply homographyh to points in p
apply_homograph&lt;-function(h,p){
  p1=t(cbind(p,1))
  q1=t(h%*%p1)
  q1=q1/q1[,3]
  q1[,1:2]
}

#Robust homography estimation from p1 to p2. Return h and the list of inliers
ransac&lt;-function(p1,p2,thresh=100,N=1000){
  n=nrow(p1)
  set.seed(12345)
  sn=c(1:n)
  flag=matrix(0,nrow=N,ncol=n)
  for(i in 1:N){
    smpl=sample(sn,4)
    pp1=p1[smpl,]
    pp2=p2[smpl,]
    h=est_homograph(pp2,pp1)
    p=apply_homograph(h,p1)
    d=rowSums((p-p2)^2)
    flag[i,]=as.numeric(d&lt;thresh)
  }
  sinliers=rowSums(flag)
  sinliers=sinliers[!is.na(sinliers)]
  imax=which(sinliers==max(sinliers))[1]
  inliers=sn[flag[imax,]==1]
  h=est_homograph(p2[inliers,],p1[inliers,])
  list(h,inliers)
}</code></pre>
<p>Now, we calculate the homography, and show the inliers in green</p>
<pre class="r"><code>hh=ransac(p1[,1:2],p2[,1:2],100,5000)

h=hh[[1]]
inliers=hh[[2]]
print(paste0(&quot;Number of inliers: &quot;,length(inliers)))</code></pre>
<pre><code>## [1] &quot;Number of inliers: 20&quot;</code></pre>
<pre class="r"><code>print(&quot;h=&quot;)</code></pre>
<pre><code>## [1] &quot;h=&quot;</code></pre>
<pre class="r"><code>print(h)</code></pre>
<pre><code>##               [,1]          [,2]        [,3]
## [1,]  3.983895e-03  8.581853e-04 0.017540363
## [2,] -8.654108e-04  3.923523e-03 0.999815300
## [3,]  2.018063e-09 -8.928259e-08 0.005380657</code></pre>
<pre class="r"><code>par(mfrow=c(1,2))

plot(ima)
kpa %$% points(mx,my,col=&quot;red&quot;)
points(p1[inliers,],col=&quot;green&quot;)
plot(imb)
kpb %$% points(mx,my,col=&quot;red&quot;)
points(p2[inliers,],col=&quot;green&quot;)</code></pre>
<p><img src="__results___files/figure-html/unnamed-chunk-11-1.png" width="960" /></p>
<p>Finally we apply the transformation to the first image and compare with the second one:</p>
<pre class="r"><code>hm1=solve(h)

 map.affine &lt;- function(x,y) {
    p=apply_homograph(hm1,cbind(x,y))
    list(x=p[,1],y=p[,2])
  }
  
nim=paste0(path,&quot;set&quot;,set,&quot;_&quot;,ida,&quot;.jpeg&quot;)
ima=imresize(as.cimg(readJPEG(nim)[,1:3099,]),scale=scl)
nim=paste0(path,&quot;set&quot;,set,&quot;_&quot;,idb,&quot;.jpeg&quot;)
imb=imresize(as.cimg(readJPEG(nim)[,1:3099,]),scale=scl)
  
imat=imwarp(ima,map=map.affine,dir=&quot;backward&quot;)

par(mfrow=c(1,2))
plot(imat)
plot(imb)</code></pre>
<p><img src="__results___files/figure-html/unnamed-chunk-12-1.png" width="960" /></p>
<p>Now, after registration, we can compare the 2 images in order to detect differences (with a well defined time arrow if possible ;) )</p>
<pre class="r"><code>d1=imat-imb
d2=(imat-imb)^2*(imat&gt;0)
par(mfrow=c(1,2))
plot(d1)
plot(log(d2+.0001))</code></pre>
<p><img src="__results___files/figure-html/unnamed-chunk-13-1.png" width="960" /></p>
<p>And that’s all for now. Probably the methodology can be fine tunned but is working prety well with most of the images. I hope this allow to the R coders to enter in the competition . Anyway, I think this workflow suggest some ways to attack the problem, build features or preprocess the images before feed them to a ML algorithm.</p>
<p>Good Image Registering!</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
