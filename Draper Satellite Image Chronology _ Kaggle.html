<!DOCTYPE html>
<!-- saved from url=(0102)https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Draper Satellite Image Chronology | Kaggle</title>
    
    <meta name="robots" content="index, follow">

    

    <link href="https://www.kaggle.com/content/v/4e3f994e938b/kaggle/favicon.ico" rel="shortcut icon" type="image/x-icon">
    <link href="./Draper Satellite Image Chronology _ Kaggle_files/css" rel="stylesheet" type="text/css">

    <link rel="stylesheet" type="text/css" href="./Draper Satellite Image Chronology _ Kaggle_files/kaggle-newworld-sitelayout.css">
    <link rel="stylesheet" type="text/css" href="./Draper Satellite Image Chronology _ Kaggle_files/kaggle-newworld.css">
    
    
    
 
    
    <script type="text/javascript" async="" src="./Draper Satellite Image Chronology _ Kaggle_files/ga.js"></script><script>
         window.intercomSettings = {
            app_id: "koj6gxx6",
            name: "Abhinav Sharma ",
            email: "abhinavs01858@gmail.com",
            created_at: 1436076958,
            user_hash: "133571e144bc505a8ee2cf9cbce3f4bbf0d1e2346728bf079047185ed750dbf2",
            "last_visit_date_at": 1466165993,  
            "is_activated": true,
            "is_locked_out": false,
            "points": 598.700012207031,
             "ranking": 10606,
             "tier": 4,
             "highest_ranking": 10606,
             "user_name": "abhinavs01858",
             "display_name": "Abhinav Sharma ",
             "is_admin": false,
             "experiment_group": 1,
             "newsletter_subscriber": true,
             "block_emails": false,
             "delete_account_reason": ""
          };
    </script>
<script>(function(){var w=window;var ic=w.Intercom;if(typeof ic==="function"){ic('reattach_activator');ic('update',intercomSettings);}else{var d=document;var i=function(){i.c(arguments)};i.q=[];i.c=function(args){i.q.push(args)};w.Intercom=i;function l(){var s=d.createElement('script');s.type='text/javascript';s.async=true;s.src='https://widget.intercom.io/widget/koj6gxx6';var x=d.getElementsByTagName('script')[0];x.parentNode.insertBefore(s,x);}if(w.attachEvent){w.attachEvent('onload',l);}else{w.addEventListener('load',l,false);}}})()</script>
</head>
<body>
    


<div id="react-SiteHeaderContainer-2"><div data-reactroot="" class="site-header-react"><div class="site-header-react__content"><div class="site-header-react__logo"><a href="https://www.kaggle.com/"><img alt="Kaggle" src="./Draper Satellite Image Chronology _ Kaggle_files/site-logo.png"></a></div><nav class="site-header-react__nav"><li class="site-header-react__nav-item"><a href="https://www.kaggle.com/competitions">Competitions</a></li><li class="site-header-react__nav-item"><a href="https://www.kaggle.com/datasets">Datasets</a></li><li class="site-header-react__nav-item"><a href="https://www.kaggle.com/scripts">Scripts</a></li><li class="site-header-react__nav-item"><a href="https://www.kaggle.com/forums">Forums</a></li><li class="site-header-react__nav-item"><a href="https://www.kaggle.com/jobs">Jobs</a></li></nav><div class="site-header-react__user"><span class="tooltip" data-tooltip="Profile &amp; community"><div class="site-header-react__user--logged-in"><ul><li class="site-header-react__user-avatar"><span class="fa fa-caret-down site-header-react__caret-down"></span><img src="./Draper Satellite Image Chronology _ Kaggle_files/f3479e3e7744f6341361fd80ae61adb0.jpg"></li></ul></div></span></div></div></div></div>
<div class="wrapper">
    <div class="main-content">
        


<div id="react-ScriptViewer-1"><div data-reactroot="" class="script-viewer"><div class="script-viewer__container"><div class="script-viewer__header"><div class="pageheader"><div class="pageheader__container"><div class="pageheader__top pageheader__top--script"><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis" class="pageheader__top-overlay pageheader__top-overlay--script"></a><div class="pageheader__top-image pageheader__top-image--script"></div><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis" class="pageheader__title pageheader__title--script">Exploratory Image Analysis</a><div class="pageheader__subtitle pageheader__subtitle--script"><p></p></div><div class="pageheader__upvote"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>83</span></div></div><div class="vote-button__show-voters"><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments">voters</a></div></div></div><div class="pageheader__info--script" style="top: 60px;"><span><!-- react-text: 23 -->by <!-- /react-text --><a href="https://www.kaggle.com/bkamphaus" class="pageheader__author-name">Ben Kamphaus</a><!-- react-text: 25 --> · <!-- /react-text --><time datetime="2016-05-11T22:05:56.917Z" title="2016-05-11T22:05:56.917Z"><!-- react-text: 27 -->last <!-- /react-text --><!-- react-text: 28 -->run<!-- /react-text --><!-- react-text: 29 --> <!-- /react-text --><span title="Thu May 12 2016 03:35:56 GMT+0530 (India Standard Time)">1 month ago</span></time></span><span><!-- react-text: 32 --> · <!-- /react-text --><!-- react-text: 33 -->Python notebook<!-- /react-text --><!-- react-text: 34 --> · <!-- /react-text --><!-- react-text: 35 -->11951<!-- /react-text --><!-- react-text: 36 --> views<!-- /react-text --></span><span><br><!-- react-text: 39 --> using data from <!-- /react-text --><a href="https://www.kaggle.com/c/draper-satellite-image-chronology/scripts">Draper Satellite Image Chronology</a></span></div></div><div><div class="pageheader__sticky pageheader__sticky--scrolling"><div class="pageheader__bottom"><div class="pageheader__author--script"><a href="https://www.kaggle.com/bkamphaus"><img src="./Draper Satellite Image Chronology _ Kaggle_files/4a002dced7ef642b40ba52c96689d43f.jpg"></a><div class="pageheader__author-script-overlay"><div class="pageheader__overlay-text">Ben Kamphaus</div></div></div><div class="pageheader__thumbnail--script"><a href="https://www.kaggle.com/c/draper-satellite-image-chronology/scripts"><img src="./Draper Satellite Image Chronology _ Kaggle_files/thumb76_76.png"></a><div class="pageheader__thumbnail-script-overlay"><div class="pageheader__overlay-text">Draper Satellite Image Chronology</div></div></div><nav class="pageheader__nav pageheader__nav--script"><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/notebook" title="Notebook" class="pageheader__left-links pageheader__left-links--with-onclick" data-link-text="Notebook">Notebook</a><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/code" title="Code" class="pageheader__left-links pageheader__left-links--with-onclick" data-link-text="Code">Code</a><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/output" title="Output" class="pageheader__left-links pageheader__left-links--with-onclick" data-link-text="Output (1)">Output (1)</a><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments" title="Comments" class="pageheader__left-links pageheader__left-links--with-onclick pageheader__left-links--selected" data-link-text="Comments (21)">Comments (21)</a><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/log" title="Log" class="pageheader__left-links pageheader__left-links--with-onclick" data-link-text="Log">Log</a><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/versions" title="Versions" class="pageheader__left-links pageheader__left-links--with-onclick" data-link-text="Versions (12)">Versions (12)</a><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/forks" title="Forks" class="pageheader__left-links pageheader__left-links--with-onclick" data-link-text="Forks (37)">Forks (37)</a></nav><a href="https://www.kaggle.com/c/draper-satellite-image-chronology/scripts/forknb/232673" title="Fork Notebook" class="pageheader__calltoaction">Fork Notebook</a></div><div></div><div class="pageheader__transition"></div></div><!-- react-text: 1414 --><!-- /react-text --></div></div></div></div><div class="script-viewer__pane-container"><div><div><div class="content-box"><!-- react-text: 741 --><!-- /react-text --><div class="content-box__title-bar"><div class="content-box__title">Output</div><div class="content-box__right-side"><div class="script-code-pane__right-section"><a href="https://www.kaggle.com/scripts/svzip/232673" class="script-code-pane__download">Download All</a></div></div></div><div class="content-box__content-section"><div class="script-files-pane"><div class="file-preview-list"><div class="file-preview-list__item"><div class="file-preview-list__name-column"><span class="fa fa-file-o"></span><span class="file-preview-list__item-filename"><a href="https://www.kaggle.io/svf/232673/c8a9b13d2bad2b7d8ceb8af8b0e9db13/results.zip">results.zip</a></span></div><div class="file-preview-list__preview-column"></div></div></div></div></div></div></div><div><div class="script-discussion-pane"><div class="content-box"><!-- react-text: 759 --><!-- /react-text --><div class="content-box__title-bar"><div class="content-box__title">Comments</div></div><div class="content-box__content-section"><div class="script-discussion-pane__messages"><div class="forum-message"><a name="118751" style="margin-top: -50px;"></a><!-- react-text: 766 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>2</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/mrshao" title="mrshao"><img src="https://secure.gravatar.com/avatar/5a001db8cb9bb2497f94bc273ad122d6.jpg?r=pg&amp;s=80&amp;d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/mrshao" title="mrshao">mrshao</a></span><span class="forum-message__time-ago"><span title="Thu May 05 2016 11:32:27 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>Nice Share</p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#118751">permalink</a></div></div></div></div><div class="forum-message"><a name="118758" style="margin-top: -50px;"></a><!-- react-text: 795 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>5</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/nigelcarpenter" title="Chippy"><img src="./Draper Satellite Image Chronology _ Kaggle_files/530168c56719cdc4abd601aa94848e29.jpg"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/nigelcarpenter" title="Chippy">Chippy</a></span><span class="forum-message__time-ago"><span title="Thu May 05 2016 12:27:09 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>Very impressive! This is exposing me to a whole new world of tools, techniques and data science. Thanks for sharing.</p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#118758">permalink</a></div></div></div></div><div class="forum-message"><a name="118766" style="margin-top: -50px;"></a><!-- react-text: 824 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>0</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/irtaza" title="Syed Irtza Ali Sha"><img src="https://secure.gravatar.com/avatar/8da7039141a849cb007662f538453eba.jpg?r=pg&amp;s=80&amp;d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/irtaza" title="Syed Irtza Ali Sha">Syed Irtza Ali Sha</a></span><span class="forum-message__time-ago"><span title="Thu May 05 2016 13:33:29 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>brilliant ! thank you for sharing</p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#118766">permalink</a></div></div></div></div><div class="forum-message"><a name="118913" style="margin-top: -50px;"></a><!-- react-text: 853 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>4</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/bkamphaus" title="Ben Kamphaus"><img src="./Draper Satellite Image Chronology _ Kaggle_files/4a002dced7ef642b40ba52c96689d43f(1).jpg"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/bkamphaus" title="Ben Kamphaus">Ben Kamphaus</a></span><span class="forum-message__time-ago"><span title="Fri May 06 2016 09:02:22 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>To be honest, I'm not really having any ideas that I think will actually be viable for the contest goal. But if I can throw out a filter or technique or something that will be helpful to someone else for stitching images together or tracking features that change between images, then I'm happy to do so. Plus, playing with imagery is fun, even when it's lacking any significant spectral dimensionality. And maybe something will spark an idea.</p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#118913">permalink</a></div></div></div></div><div class="forum-message"><a name="118915" style="margin-top: -50px;"></a><!-- react-text: 882 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>2</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/the1owl" title="the1owl"><img src="./Draper Satellite Image Chronology _ Kaggle_files/2664b23f76b5fc9c3f767e0d3577f561.jpg"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/the1owl" title="the1owl">the1owl</a></span><span class="forum-message__time-ago"><span title="Fri May 06 2016 09:12:27 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>This is a better script tutorial than many found on the web today because it is not focused on a single module or method. The broad spectrum of ideas introduced will be great when blended together with further methods.  I am not at a Neural Net stage yet but will surely incorporate your ideas on a couple of public scripts.</p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#118915">permalink</a></div></div></div></div><div class="forum-message"><a name="119055" style="margin-top: -50px;"></a><!-- react-text: 911 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>1</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/nigelcarpenter" title="Chippy"><img src="./Draper Satellite Image Chronology _ Kaggle_files/530168c56719cdc4abd601aa94848e29.jpg"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/nigelcarpenter" title="Chippy">Chippy</a></span><span class="forum-message__time-ago"><span title="Sat May 07 2016 04:26:38 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>Ben do you think you'd be able get image matching working? That would require you to determine image rotation and zoom levels between images from the same set. I think that rotation and zoom info could be very helpful in achieving the contest goal.</p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#119055">permalink</a></div></div></div></div><div class="forum-message"><a name="119082" style="margin-top: -50px;"></a><!-- react-text: 940 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>1</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/bkamphaus" title="Ben Kamphaus"><img src="./Draper Satellite Image Chronology _ Kaggle_files/4a002dced7ef642b40ba52c96689d43f(1).jpg"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/bkamphaus" title="Ben Kamphaus">Ben Kamphaus</a></span><span class="forum-message__time-ago"><span title="Sat May 07 2016 08:26:33 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>@Chippy</p>

<p>Most of my experience with tackling registration is with proprietary software. So at present, I'm mainly reading up on techniques from various sources e.g.<a href="http://research.microsoft.com/pubs/75695/Szeliski-FnT06.pdf">here</a>. There's some python source for an FFT approach out there, <a href="http://www.lfd.uci.edu/~gohlke/code/imreg.py.html">link</a>, packaged <a href="https://pypi.python.org/pypi/imreg_dft/">here</a>, but looks like it may not be robust to changes in scale and I'm not sure what the mim/max scale changes are across the dataset.</p>

<p>I'm really just loosely reading through a few things at the moment hoping to find something at an API level or that can be implemented trivially with vectorized routines in numpy/scipy world. Anything more involved will probably force me to code it up locally in Julia. To be honest, your scripts for homography estimation and AKAZE keypoint matching are several steps ahead of my thinking on this already.</p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#119082">permalink</a></div></div></div></div><div class="forum-message"><a name="119216" style="margin-top: -50px;"></a><!-- react-text: 969 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>1</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/wordsforthewise" title="NathanGeorge"><img src="https://secure.gravatar.com/avatar/44312619f6a5177d6679898c4391872f.jpg?r=pg&amp;s=80&amp;d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/wordsforthewise" title="NathanGeorge">NathanGeorge</a></span><span class="forum-message__message-edit-notice"><!-- react-text: 990 --> · last edited <!-- /react-text --><span title="Tue May 10 2016 11:51:19 GMT+0530 (India Standard Time)">1 month ago</span><!-- react-text: 992 --> by <!-- /react-text --><a href="https://www.kaggle.com/wordsforthewise">NathanGeorge</a></span><span class="forum-message__time-ago"><span title="Sun May 08 2016 11:06:35 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>Thanks for this wealth of information!  I'm having one issue though, the normalized images show up as all black, and my normalized spectral distributions look different than yours.  I can't yet figure out why the normalization function isn't working.</p>

<p>To answer my own question: there is a '.astype(np.float32)' missing in the normalize function.</p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#119216">permalink</a></div></div></div></div><div class="forum-message"><a name="119223" style="margin-top: -50px;"></a><!-- react-text: 1003 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>0</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/thomashall" title="Terrible Machine"><img src="https://secure.gravatar.com/avatar/423f2a2014498e9d716620b7907051f8.jpg?r=pg&amp;s=80&amp;d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/thomashall" title="Terrible Machine">Terrible Machine</a></span><span class="forum-message__time-ago"><span title="Sun May 08 2016 13:24:38 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>Thank you</p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#119223">permalink</a></div></div></div></div><div class="forum-message"><a name="119256" style="margin-top: -50px;"></a><!-- react-text: 1032 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>2</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/rockon" title="Brian"><img src="https://secure.gravatar.com/avatar/1bd1943a363f3c5d42eeb480b1f94a5b.jpg?r=pg&amp;s=80&amp;d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/rockon" title="Brian">Brian</a></span><span class="forum-message__time-ago"><span title="Sun May 08 2016 21:19:20 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>Nice work, Ben.  Are you familiar with the work of Vladik Kreinovich at UTEP?  One of the thesis committees I was on with Vladik was for one of his graduate students, Steve Gibson, involving the use of <a href="http://www.cs.utep.edu/vladik/2000/tr00-02a.pdf">FFTs</a> for image registration and mosaicking.  </p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#119256">permalink</a></div></div></div></div><div class="forum-message"><a name="119516" style="margin-top: -50px;"></a><!-- react-text: 1061 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>0</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/bkamphaus" title="Ben Kamphaus"><img src="./Draper Satellite Image Chronology _ Kaggle_files/4a002dced7ef642b40ba52c96689d43f(1).jpg"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/bkamphaus" title="Ben Kamphaus">Ben Kamphaus</a></span><span class="forum-message__time-ago"><span title="Wed May 11 2016 10:35:23 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>@Brian</p>

<p>I've aware of (and attempting approaches based on) <a href="http://digitalcommons.utep.edu/cgi/viewcontent.cgi?article=1355&amp;context=cs_techrep">http://digitalcommons.utep.edu/cgi/viewcontent.cgi?article=1355&amp;context=cs_techrep</a> -- which I now see he's a co-author on.</p>

<p>@NathanGeorge you're correct, I believe I should review/check  data type use, though maybe you're running into the Python2 and Python3 different interpretation of <code>/</code> as defaulting to floor division or not? But listing type explicitly will make code more portable anyways.</p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#119516">permalink</a></div></div></div></div><div class="forum-message"><a name="119522" style="margin-top: -50px;"></a><!-- react-text: 1090 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>0</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/wordsforthewise" title="NathanGeorge"><img src="https://secure.gravatar.com/avatar/44312619f6a5177d6679898c4391872f.jpg?r=pg&amp;s=80&amp;d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/wordsforthewise" title="NathanGeorge">NathanGeorge</a></span><span class="forum-message__time-ago"><span title="Wed May 11 2016 12:04:31 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>Ah, must by a Python 2 thing, that's what I'm using.</p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#119522">permalink</a></div></div></div></div><div class="forum-message"><a name="119543" style="margin-top: -50px;"></a><!-- react-text: 1119 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>0</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/yourwanghao" title="HawkWang"><img src="https://secure.gravatar.com/avatar/e05bf68a9a799432702cc3e6fa8967cd.jpg?r=pg&amp;s=80&amp;d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/yourwanghao" title="HawkWang">HawkWang</a></span><span class="forum-message__time-ago"><span title="Wed May 11 2016 14:43:20 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>@Ben, it is obvious that the registration is failed. I think it is because of the limitation of imreg.py, which can't handle so much rotation/scale/translation/and other difference of the input images.  Maybe we should consider other mechanism to do image registry.  But the question is why we should do that? What do we want from the transformation?  </p>

<p>I am thinking maybe we should compute the similarity value between a query image and the images from different Day group.  So that we know which Day group this image belongs to.  </p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#119543">permalink</a></div></div></div></div><div class="forum-message"><a name="119565" style="margin-top: -50px;"></a><!-- react-text: 1148 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>0</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/chabir" title="eagle4"><img src="https://secure.gravatar.com/avatar/67fce88939354a4f8e7b837d336d5d3a.jpg?r=pg&amp;s=80&amp;d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/chabir" title="eagle4">eagle4</a></span><span class="forum-message__time-ago"><span title="Wed May 11 2016 18:05:45 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>@ Ben: your script is really helpful.</p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#119565">permalink</a></div></div></div></div><div class="forum-message"><a name="119582" style="margin-top: -50px;"></a><!-- react-text: 1177 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>0</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/bkamphaus" title="Ben Kamphaus"><img src="./Draper Satellite Image Chronology _ Kaggle_files/4a002dced7ef642b40ba52c96689d43f(1).jpg"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/bkamphaus" title="Ben Kamphaus">Ben Kamphaus</a></span><span class="forum-message__time-ago"><span title="Wed May 11 2016 20:38:49 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>@HawkWang the idea is to be able to calculate meaningful difference images. You're also right that it would make sense to just use the transform calculated or similarity information.</p>

<p>I'm also considering a deep learning binary classifier approach to just classify which image is first/later given two images. I would still want some way to estimate image overlap so I could take lots and lots of overlapping subsamples of the image pair per every image pair to build up a large training set. I'm just working off the assumption that it would take a lot of data to get something that would generalize from training images to test images.</p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#119582">permalink</a></div></div></div></div><div class="forum-message"><a name="119633" style="margin-top: -50px;"></a><!-- react-text: 1206 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>0</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/yourwanghao" title="HawkWang"><img src="https://secure.gravatar.com/avatar/e05bf68a9a799432702cc3e6fa8967cd.jpg?r=pg&amp;s=80&amp;d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/yourwanghao" title="HawkWang">HawkWang</a></span><span class="forum-message__time-ago"><span title="Thu May 12 2016 06:30:29 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>@Ben, good idea for deep learning. But why not just use classifier for 5 classes, instead of the binary classifier?
If we want to use deep learning we must consider the input data format and size. It is good to use the overlapped area to form some small ROI patches as input. </p>

<p>By the way, I just tried python's phash to compute the similarity between one set images to others. (I tried set107). But it seems phash's result is wrong, at least the distance can't be used directly to find the order. Maybe you can have a try. </p>

<p>I will go on to use SIFT features descriptor to try again.</p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#119633">permalink</a></div></div></div></div><div class="forum-message"><a name="119640" style="margin-top: -50px;"></a><!-- react-text: 1235 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>0</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/bkamphaus" title="Ben Kamphaus"><img src="./Draper Satellite Image Chronology _ Kaggle_files/4a002dced7ef642b40ba52c96689d43f(1).jpg"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/bkamphaus" title="Ben Kamphaus">Ben Kamphaus</a></span><span class="forum-message__time-ago"><span title="Thu May 12 2016 07:04:28 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>@HawkWang In this case, I don't think a softmax layer classifier is the correct approach. There's no underlying "firstness" or "secondness" in the domain unless it is some sort of information leak (i.e., sensors all captures the image on the same day in the same hemisphere). What we really want to be able to do is to be able to sort images arbitrarily in the order they were captured. And that, to me, implies training a neural network to be an image comparator - that is, feed in two images and have it output which of those images is later.</p>

<p>A sparse or distributed representation, like what a deep net will learn, is appropriate for e.g. first detecting the character of the illumination or vegetation in the scenes then looking at differences in combination with that. This means there's a possibility (at least in my mind) that it could learn this comparator function.</p>

<p>I'm not completely sure yet whether images or image patches <em>do</em> need to be co-aligned. It may be that cars moving, etc. may end up just contributing to the noise (i.e. period of within day rather than between days), and that changes in illumination, vegetation, etc. which are more robust features within a year will be better contributors and could potentially generalize ok in being applied between scenes that aren't registered.</p>

<p>Just as a side note, the feasibility of a fully automated solution to the challenge problem is still an open question in my mind. But I'm not really interested in any other take on it. Laziness, impatience, and hubris will keep me from taking a stab at any manual labeling. :)</p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#119640">permalink</a></div></div></div></div><div class="forum-message"><a name="119645" style="margin-top: -50px;"></a><!-- react-text: 1264 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>0</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/itodorovic" title="itod"><img src="./Draper Satellite Image Chronology _ Kaggle_files/2a365642c2bc38e0bfba16ade9f70906.jpg"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/itodorovic" title="itod">itod</a></span><span class="forum-message__time-ago"><span title="Thu May 12 2016 07:37:12 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>I was also thinking about the cars problem adding to the noise and wrong conclusions. The only solution that came to mind is to train separate neural net to recognize the roads and replace them with black pixels accros all images, but that would require a ton of manual labeling.</p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#119645">permalink</a></div></div></div></div><div class="forum-message"><a name="119649" style="margin-top: -50px;"></a><!-- react-text: 1293 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>1</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/yourwanghao" title="HawkWang"><img src="https://secure.gravatar.com/avatar/e05bf68a9a799432702cc3e6fa8967cd.jpg?r=pg&amp;s=80&amp;d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/yourwanghao" title="HawkWang">HawkWang</a></span><span class="forum-message__time-ago"><span title="Thu May 12 2016 08:47:53 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>@Ben, I am also only interested in a fully automatic way on this task. And I am still thinking if we want to see the order, or change, in the images, we only need to consider the aligned, or normalized, public area of the images. </p>

<p>But, do you notice, that set4 day 3 is after a raining? I can see that the upper left corner ground is wet, which is different from the Day 1 and Day 2.  Even we want an automatic way, we may need to use this human sensed information.</p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#119649">permalink</a></div></div></div></div><div class="forum-message"><a name="120203" style="margin-top: -50px;"></a><!-- react-text: 1322 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>0</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/liwste" title="liwste"><img src="https://secure.gravatar.com/avatar/4f60eab5e545c1b6a6d11039978fddf8.jpg?r=pg&amp;s=80&amp;d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/liwste" title="liwste">liwste</a></span><span class="forum-message__time-ago"><span title="Mon May 16 2016 16:05:25 GMT+0530 (India Standard Time)">1 month ago</span></span></div></div><div class="forum-message__message-content"><p>Is it better to use PIL to read in the image files rather than matplotlib ?</p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#120203">permalink</a></div></div></div></div><div class="forum-message"><a name="122422" style="margin-top: -50px;"></a><!-- react-text: 1351 --> <!-- /react-text --><div class="forum-message__info-side"><div class="forum-message__vote-section"><div class="vote-button__container"><div class="vote-button vote-button--enabled"><div><span class="fa fa-caret-up vote-button__caret-up"></span></div><div class="vote-button__vote-count"><span>0</span></div><div><span class="fa fa-caret-down vote-button__caret-down"></span></div></div></div></div></div><div class="forum-message__content-side"><div class="forum-message__content-top"><div class="forum-message__content-upper-row"><div class="forum-message__avatar"><a href="https://www.kaggle.com/ww44ss" title="WW44SS"><img src="https://secure.gravatar.com/avatar/3176fb6712eeafe13e2745cb59d1360d.jpg?r=pg&amp;s=80&amp;d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png"></a></div><div class="forum-message__message-metadata"><span class="forum-message__author"><a href="https://www.kaggle.com/ww44ss" title="WW44SS">WW44SS</a></span><span class="forum-message__time-ago"><span title="Sat Jun 04 2016 05:01:29 GMT+0530 (India Standard Time)">2 weeks ago</span></span></div></div><div class="forum-message__message-content"><p>Thank you. This is great. </p></div></div><div class="forum-message__content-bottom"><div class="forum-message__action-section"><span> </span><a href="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/comments#122422">permalink</a></div></div></div></div></div><div class="script-discussion-pane__comment-editor"><div class="markdown-editor"><div tabindex="0" class="wmd-panel"><div id="wmd-button-bared"><ul id="wmd-button-rowed" class="wmd-button-row"><li class="wmd-button" id="wmd-bold-buttoned" title="Strong &lt;strong&gt; Ctrl+B" style="left: 0px;"><span style="background-position: 0px 0px;"></span></li><li class="wmd-button" id="wmd-italic-buttoned" title="Emphasis &lt;em&gt; Ctrl+I" style="left: 25px;"><span style="background-position: -20px 0px;"></span></li><li class="wmd-spacer wmd-spacer1" id="wmd-spacer1ed"></li><li class="wmd-button" id="wmd-link-buttoned" title="Hyperlink &lt;a&gt; Ctrl+L" style="left: 75px;"><span style="background-position: -40px 0px;"></span></li><li class="wmd-button" id="wmd-quote-buttoned" title="Blockquote &lt;blockquote&gt; Ctrl+Q" style="left: 100px;"><span style="background-position: -60px 0px;"></span></li><li class="wmd-button" id="wmd-code-buttoned" title="Code Sample &lt;pre&gt;&lt;code&gt; Ctrl+K" style="left: 125px;"><span style="background-position: -80px 0px;"></span></li><li class="wmd-button" id="wmd-image-buttoned" title="Image &lt;img&gt; Ctrl+G" style="left: 150px;"><span style="background-position: -100px 0px;"></span></li><li class="wmd-spacer wmd-spacer2" id="wmd-spacer2ed"></li><li class="wmd-button" id="wmd-olist-buttoned" title="Numbered List &lt;ol&gt; Ctrl+O" style="left: 200px;"><span style="background-position: -120px 0px;"></span></li><li class="wmd-button" id="wmd-ulist-buttoned" title="Bulleted List &lt;ul&gt; Ctrl+U" style="left: 225px;"><span style="background-position: -140px 0px;"></span></li><li class="wmd-button" id="wmd-heading-buttoned" title="Heading &lt;h1&gt;/&lt;h2&gt; Ctrl+H" style="left: 250px;"><span style="background-position: -160px 0px;"></span></li><li class="wmd-button" id="wmd-hr-buttoned" title="Horizontal Rule &lt;hr&gt; Ctrl+R" style="left: 275px;"><span style="background-position: -180px 0px;"></span></li><li class="wmd-spacer wmd-spacer3" id="wmd-spacer3ed"></li><li class="wmd-button" id="wmd-undo-buttoned" title="Undo - Ctrl+Z" style="left: 325px;"><span style="background-position: -200px -20px;"></span></li><li class="wmd-button" id="wmd-redo-buttoned" title="Redo - Ctrl+Y" style="left: 350px;"><span style="background-position: -220px -20px;"></span></li></ul></div><a href="http://commonmark.org/help/" target="__blank" class="markdown-editor__help"><img src="./Draper Satellite Image Chronology _ Kaggle_files/markdown.png"><!-- react-text: 1384 --> Styling with Markdown supported<!-- /react-text --></a><textarea name="rawMarkdown" class="wmd-input" id="wmd-inputed" placeholder="Enter your comments." style="height: 112.222px;"></textarea></div><div id="wmd-previewed" class="wmd-panel wmd-preview post-text-styling"></div></div><div class="script-discussion-pane__comment-editor-button"><a class="button__anchor-wrapper"><div class="button"><span>Post Reply</span></div></a></div></div></div></div></div></div></div><div><div class="script-notebook-pane" style="display: none;"><div class="content-box"><!-- react-text: 70 --><!-- /react-text --><div class="content-box__title-bar"><div class="content-box__title">Notebook</div></div><div class="content-box__content-section"><div class="script-notebook-pane__container"><iframe src="./Draper Satellite Image Chronology _ Kaggle_files/__results__.html" style="height: 22174px;"></iframe><div class="script-notebook-pane__loading" style="display: none;"><div class="script-notebook-pane__loading-content"><span class="fa fa-circle-o-notch fa-spin"></span></div></div></div></div></div></div></div></div></div></div></div>


<form action="https://www.kaggle.com/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis" id="__AjaxAntiForgeryForm" method="post"><input name="__RequestVerificationToken" type="hidden" value="3V06YlHbEBPQgV-p2ZkdAQt4jyOKl_9o-pQo2z-IG7OxWln-mxenJ1gV6NXz3N3s8ZwykJFBWnbgOzJ8F1B7Az69bN81"></form>
<script type="text/javascript" src="./Draper Satellite Image Chronology _ Kaggle_files/kaggle.prism.js"></script>
    </div>
    <footer class="site-footer">
    <div class="site-footer__content">
        <div class="site-footer__copyright">
            <span>© 2016 Kaggle Inc</span>
        </div>
        <nav class="site-footer__nav">
            <a href="https://www.kaggle.com/team">Our Team</a>
            <a href="https://www.kaggle.com/careers">Careers</a>
            <a href="https://www.kaggle.com/terms">Terms</a>
            <a href="https://www.kaggle.com/privacy">Privacy</a>
            <a href="https://www.kaggle.com/Home/contact">Contact/Support</a>
        </nav>
        <nav class="site-footer__social">
            <a class="site-footer__social--twitter" href="http://www.twitter.com/kaggle" title="Follow Kaggle on Twitter"></a>
            <a class="site-footer__social--facebook" href="http://www.facebook.com/kaggle" title="Follow Kaggle on Facebook"></a>
            <a class="site-footer__social--linkedin" href="http://www.linkedin.com/companies/kaggle" title="Follow Kaggle on LinkedIn"></a>
        </nav>
    </div>
</footer>


</div>


<script type="text/javascript">
    var Kaggle = Kaggle || {};
    Kaggle.Current = {
            userId: 378178,
            userProfileUrl: '/abhinavs01858',
            userDisplayName: 'Abhinav Sharma ',
            userThumbnailUrl: 'https://secure.gravatar.com/avatar/f3479e3e7744f6341361fd80ae61adb0.jpg?r=pg&amp;s=90&amp;d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png',
        isAnonymous: false
    }
</script>



<script type="text/javascript" src="./Draper Satellite Image Chronology _ Kaggle_files/jquery-1.7.2.min.js"></script>
<script type="text/javascript" src="./Draper Satellite Image Chronology _ Kaggle_files/react.min.js"></script>
<script type="text/javascript" src="./Draper Satellite Image Chronology _ Kaggle_files/react-dom.min.js"></script>
<script type="text/javascript" src="./Draper Satellite Image Chronology _ Kaggle_files/react-components.js"></script><script type="text/javascript">
ReactDOM.render(React.createElement(KaggleReactComponents.ScriptViewer, {"pageMessages":[],"log":"[TerminalIPythonApp] WARNING | Subcommand `ipython nbconvert` is deprecated and will be removed in future versions.\n[TerminalIPythonApp] WARNING | You likely want to use `jupyter nbconvert`... continue in 5 sec. Press Ctrl-C to quit now.\n[NbConvertApp] WARNING | Unrecognized JSON config file version, assuming version 1\n[NbConvertApp] Converting notebook script.ipynb to html\n[NbConvertApp] Executing notebook with kernel: python3\n/opt/conda/lib/python3.5/site-packages/IPython/paths.py:69: UserWarning: IPython parent \u0027/\u0027 is not a writable location, using a temp directory.\n  \u0022 using a temp directory.\u0022.format(parent))\n[NbConvertApp] Support files will be in __results___files/\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Making directory __results___files\n[NbConvertApp] Writing 395512 bytes to __results__.html\u0022},","code":"{\u0022cells\u0022:[\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport sklearn\\nimport glob, os\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022# Read in files\\n\\nThis is pretty routine stuff.\\n\\n* We get a list of jpeg files, reading them in as needed with `matplotlib.pyplot.imread`.\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022from subprocess import check_output\\nprint(check_output([\\\u0022ls\\\u0022, \\\u0022../input\\\u0022]).decode(\\\u0022utf8\\\u0022))\\nsmjpegs = [f for f in glob.glob(\\\u0022../input/train_sm/*.jpeg\\\u0022)]\\nprint(smjpegs[:9])\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022set175 = [smj for smj in smjpegs if \\\u0022set175\\\u0022 in smj]\\nprint(set175)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022# Basic exploration\\n\\nJust look at image dimensions, confirm it\u0027s 3 band (RGB), byte scaled (0-255).\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022first = plt.imread(\u0027../input/train_sm/set175_1.jpeg\u0027)\\ndims = np.shape(first)\\nprint(dims)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022np.min(first), np.max(first)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022For any image specific classification, clustering, etc. transforms we\u0027ll want to \\ncollapse spatial dimensions so that we have a matrix of pixels by color channels.\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022pixel_matrix = np.reshape(first, (dims[0] * dims[1], dims[2]))\\nprint(np.shape(pixel_matrix))\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022Scatter plots are a go to to look for clusters and separatbility in the data, but these are busy and don\u0027t reveal density well, so we\\nswitch to using 2d histograms instead. The data between bands is really correlated, typical with\\nvisible imagery and why most satellite image analysts prefer to at least have near infrared values.\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022#plt.scatter(pixel_matrix[:,0], pixel_matrix[:,1])\\n_ = plt.hist2d(pixel_matrix[:,1], pixel_matrix[:,2], bins=(50,50))\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022fifth = plt.imread(\u0027../input/train_sm/set175_5.jpeg\u0027)\\ndims = np.shape(fifth)\\npixel_matrix5 = np.reshape(fifth, (dims[0] * dims[1], dims[2]))\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022_ = plt.hist2d(pixel_matrix5[:,1], pixel_matrix5[:,2], bins=(50,50))\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022We can look at variations between the scenes now and see that there\u0027s a significant\\namount of difference, probably due to sensor angle and illumination variation. Raw band\\ndifferences will need to be scaled or thresholded for any traditional approach.\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022_ = plt.hist2d(pixel_matrix[:,2], pixel_matrix5[:,2], bins=(50,50))\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022plt.imshow(first)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022plt.imshow(fifth)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022Without coregistering portions of the image, the naive red band subtraction for change indication\\nbasically just shows the location shift between images.\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022plt.imshow(first[:,:,2] - fifth[:,:,1])\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022second = plt.imread(\u0027../input/train_sm/set175_2.jpeg\u0027)\\nplt.imshow(first[:,:,2] - second[:,:,2])\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022plt.imshow(second)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022# Initial impressions\\n\\nImages aren\u0027t registered, so an image registration process between images with common overlap would probably be the first step in a traditional approach.\\nUsing a localizer in a deep learning context would probably be the newfangled way to tackle this.\\n\\nImage content and differences will be dominated by topographic and built variations\\ndue to sensor orientation, resolution differences between scenes, and some registration accuracy will be impossible to factor out as\\nthe image hasn\u0027t been orthorectified and some anciliary data would be required for it\\nto be done, e.g. georeferenceing against a previously orthorectified image.\\n\\nSo this is basically a basic computer vision task that deep learning will be a good fit for. The usual preprocessing steps\\nand data expectations you\u0027d see in remote sensing aren\u0027t fulfilled by this dataset.\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022# simple k means clustering\\nfrom sklearn import cluster\\n\\nkmeans = cluster.KMeans(5)\\nclustered = kmeans.fit_predict(pixel_matrix)\\n\\ndims = np.shape(first)\\nclustered_img = np.reshape(clustered, (dims[0], dims[1]))\\nplt.imshow(clustered_img)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022plt.imshow(first)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022ind0, ind1, ind2, ind3 = [np.where(clustered == x)[0] for x in [0, 1, 2, 3]]\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022This code doesn\u0027t run on the server.\\n\\n```python\\nfrom mpl_toolkits.mplot3d import Axes3D\\n\\nfig = plt.figure()\\nax = fig.add_subplot(111, projection=\u00273d\u0027)\\n\\nplot_vals = [(\u0027r\u0027, \u0027o\u0027, ind0),\\n             (\u0027b\u0027, \u0027^\u0027, ind1),\\n             (\u0027g\u0027, \u00278\u0027, ind2),\\n             (\u0027m\u0027, \u0027*\u0027, ind3)]\\n\\nfor c, m, ind in plot_vals:\\n    xs = pixel_matrix[ind, 0]\\n    ys = pixel_matrix[ind, 1]\\n    zs = pixel_matrix[ind, 2]\\n    ax.scatter(xs, ys, zs, c=c, marker=m)\\n\\nax.set_xlabel(\u0027Blue channel\u0027)\\nax.set_ylabel(\u0027green channel\u0027)\\nax.set_zlabel(\u0027Red channel\u0027)\\n```\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022# quick look at color value histograms for pixel matrix from first image\\nimport seaborn as sns\\nsns.distplot(pixel_matrix[:,0], bins=12)\\nsns.distplot(pixel_matrix[:,1], bins=12)\\nsns.distplot(pixel_matrix[:,2], bins=12)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022# even subsampling is throwing memory error for me, :p\\n#length = np.shape(pixel_matrix)[0]\\n#rand_ind = np.random.choice(length, size=50000)\\n#sns.pairplot(pixel_matrix[rand_ind,:])\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022# Day 2\\n\\nWe\u0027ll start by considering the entire sequence of a different image set this time and look at strategies\\nfor matching features across scenes.\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022set79 = [smj for smj in smjpegs if \\\u0022set79\\\u0022 in smj]\\nprint(set79)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022img79_1, img79_2, img79_3, img79_4, img79_5 = \\\\\\n  [plt.imread(\\\u0022../input/train_sm/set79_\\\u0022 + str(n) + \\\u0022.jpeg\\\u0022) for n in range(1, 6)]\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022img_list = (img79_1, img79_2, img79_3, img79_4, img79_5)\\n\\nplt.figure(figsize=(8,10))\\nplt.imshow(img_list[0])\\nplt.show()\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022Tracking dimensions across image transforms is annoying, so we\u0027ll make a class to do that.\\nAlso I\u0027m going to use this brightness normalization transform and visualize the image that\\nway, good test scenario for class.\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022class MSImage():\\n    \\\u0022\\\u0022\\\u0022Lightweight wrapper for handling image to matrix transforms. No setters,\\n    main point of class is to remember image dimensions despite transforms.\\\u0022\\\u0022\\\u0022\\n    \\n    def __init__(self, img):\\n        \\\u0022\\\u0022\\\u0022Assume color channel interleave that holds true for this set.\\\u0022\\\u0022\\\u0022\\n        self.img = img\\n        self.dims = np.shape(img)\\n        self.mat = np.reshape(img, (self.dims[0] * self.dims[1], self.dims[2]))\\n\\n    @property\\n    def matrix(self):\\n        return self.mat\\n        \\n    @property\\n    def image(self):\\n        return self.img\\n    \\n    def to_flat_img(self, derived):\\n        \\\u0022\\\u0022\\\u0022\\\u0022Use dims property to reshape a derived matrix back into image form when\\n        derived image would only have one band.\\\u0022\\\u0022\\\u0022\\n        return np.reshape(derived, (self.dims[0], self.dims[1]))\\n    \\n    def to_matched_img(self, derived):\\n        \\\u0022\\\u0022\\\u0022\\\u0022Use dims property to reshape a derived matrix back into image form.\\\u0022\\\u0022\\\u0022\\n        return np.reshape(derived, (self.dims[0], self.dims[1], self.dims[2]))\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022msi79_1 = MSImage(img79_1)\\nprint(np.shape(msi79_1.matrix))\\nprint(np.shape(msi79_1.img))\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022# Brightness Normalization\\n\\nBrightness Normalization is preprocessing strategy you can apply prior to using strategies\\nto identify materials in a scene, if you want your matching algorithm\\nto be robust across variations in illumination. See [Wu\u0027s paper](https://pantherfile.uwm.edu/cswu/www/my%20publications/2004_RSE.pdf).\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022def bnormalize(mat):\\n    \\\u0022\\\u0022\\\u0022much faster brightness normalization, since it\u0027s all vectorized\\\u0022\\\u0022\\\u0022\\n    bnorm = np.zeros_like(mat, dtype=np.float32)\\n    maxes = np.max(mat, axis=1)\\n    bnorm = mat / np.vstack((maxes, maxes, maxes)).T\\n    return bnorm\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022bnorm = bnormalize(msi79_1.matrix)\\nbnorm_img = msi79_1.to_matched_img(bnorm)\\nplt.figure(figsize=(8,10))\\nplt.imshow(bnorm_img)\\nplt.show()\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022msi79_2 = MSImage(img79_2)\\nbnorm79_2 = bnormalize(msi79_2.matrix)\\nbnorm79_2_img = msi79_2.to_matched_img(bnorm79_2)\\nplt.figure(figsize=(8,10))\\nplt.imshow(bnorm79_2_img)\\nplt.show()\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022msinorm79_1 = MSImage(bnorm_img)\\nmsinorm79_2 = MSImage(bnorm79_2_img)\\n\\n_ = plt.hist2d(msinorm79_1.matrix[:,2], msinorm79_2.matrix[:,2], bins=(50,50))\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022_ = plt.hist2d(msinorm79_1.matrix[:,1], msinorm79_2.matrix[:,1], bins=(50,50))\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022_ = plt.hist2d(msinorm79_1.matrix[:,0], msinorm79_2.matrix[:,0], bins=(50,50))\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022import seaborn as sns\\nsns.distplot(msinorm79_1.matrix[:,0], bins=12)\\nsns.distplot(msinorm79_1.matrix[:,1], bins=12)\\nsns.distplot(msinorm79_1.matrix[:,2], bins=12)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022plt.figure(figsize=(8,10))\\nplt.imshow(img79_1)\\nplt.show()\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022np.max(img79_1[:,:,0])\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022# Using thresholds with brightness normalization\\n\\nOk, so what am I even doing here? Well, my goal is to try and figure out simple threshold selection\\nmethods for getting high albedo targets out of a scene so I could then theoretically track them\\nbetween scenes. For example, a simple blob/aggregation to centroid (in coordinates or in subsampled\\nimage bins) would give me a means to look at plausible structural similarities in distributions\\nbetween scenes, then use that to anchor a comparison of things that change.\\n\\nThe brightness normalization step is helpful because thresholds that aren\u0027t anchored by a\\npreprocessing step end up being arbitrary and can\u0027t generalize between scenes even in the same\\nimage set, whereas thresholds following brightness normalization tend to pull out materils that stand\\nout from the background more reliably. See the following demonstration:\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022plt.figure(figsize=(10,15))\\nplt.subplot(121)\\nplt.imshow(img79_1[:,:,0] \u003e 230)\\nplt.subplot(122)\\nplt.imshow(img79_1)\\nplt.show()\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022plt.figure(figsize=(10,15))\\nplt.subplot(121)\\nplt.imshow(img79_2[:,:,0] \u003e 230)\\nplt.subplot(122)\\nplt.imshow(img79_2)\\nplt.show()\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022print(np.min(bnorm79_2_img[:,:,0]))\\nprint(np.max(bnorm79_2_img[:,:,0]))\\nprint(np.mean(bnorm79_2_img[:,:,0]))\\nprint(np.std(bnorm79_2_img[:,:,0]))\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022plt.figure(figsize=(10,15))\\nplt.subplot(121)\\nplt.imshow(bnorm79_2_img[:,:,0] \u003e 0.98)\\nplt.subplot(122)\\nplt.imshow(img79_2)\\nplt.show()\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022plt.figure(figsize=(10,15))\\nplt.subplot(121)\\nplt.imshow(bnorm_img[:,:,0] \u003e 0.98)\\nplt.subplot(122)\\nplt.imshow(img79_1)\\nplt.show()\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022plt.figure(figsize=(10,15))\\nplt.subplot(121)\\nplt.imshow((bnorm79_2_img[:,:,0] \u003e 0.9999) \u0026 \\\\\\n           (bnorm79_2_img[:,:,1] \u003c 0.9999) \u0026 \\\\\\n           (bnorm79_2_img[:,:,2] \u003c 0.9999))\\nplt.subplot(122)\\nplt.imshow(img79_2)\\nplt.show()\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022plt.figure(figsize=(10,15))\\nplt.subplot(121)\\nplt.imshow(bnorm_img[:,:,0] \u003e 0.995)\\nplt.subplot(122)\\nplt.imshow(img79_1)\\nplt.show()\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022plt.figure(figsize=(10,6))\\nplt.subplot(121)\\nplt.plot(bnorm_img[2000, 1000, :])\\nplt.subplot(122)\\nplt.plot(img79_1[2000, 1000, :])\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022from scipy import spatial\\n\\npixel = msi79_1.matrix[2000 * 1000, :]\\nnp.shape(pixel)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022# Something\u0027s borked here\\n\\nThink I\u0027m gonna have to verify cosine similarity behavior for scipy here.\\n\\n```python\\ndef spectral_angle_mapper(pixel):\\n    return lambda p2: spatial.distance.cosine(pixel, p2)\\n\\nmatch_pixel = np.apply_along_axis(spectral_angle_mapper(pixel), 1, msi79_1.matrix)\\n\\nplt.figure(figsize=(10,6))\\nplt.imshow(msi79_1.to_flat_img(match_pixel \u003c 0.0000001))\\n\\ndef summary(mat):\\n    print(\\\u0022Max: \\\u0022, np.max(mat),\\n          \\\u0022Min: \\\u0022, np.min(mat),\\n          \\\u0022Std: \\\u0022, np.std(mat),\\n          \\\u0022Mean: \\\u0022, np.mean(mat))\\n    \\nsummary(match_pixel)\\n```\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022# Rudimentary Transforms, Edge Detection, Texture\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022set144 = [MSImage(plt.imread(smj)) for smj in smjpegs if \\\u0022set144\\\u0022 in smj]\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022plt.imshow(set144[0].image)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022import skimage\\nfrom skimage.feature import greycomatrix, greycoprops\\nfrom skimage.filters import sobel\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022# Sobel Edge Detection\\n\\nA Sobel filter is one means of getting a basic edge magnitude/gradient image. Can be useful to\\nthreshold and find prominent linear features, etc. Several other similar filters in skimage.filters\\nare also good edge detectors: `roberts`, `scharr`, etc. and you can control direction, i.e. use\\nan anisotropic version.\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022# a sobel filter is a basic way to get an edge magnitude/gradient image\\nfig = plt.figure(figsize=(8, 8))\\nplt.imshow(sobel(set144[0].image[:750,:750,2]))\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022from skimage.filters import sobel_h\\n\\n# can also apply sobel only across one direction.\\nfig = plt.figure(figsize=(8, 8))\\nplt.imshow(sobel_h(set144[0].image[:750,:750,2]), cmap=\u0027BuGn\u0027)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022from sklearn.decomposition import PCA\\n\\npca = PCA(3)\\npca.fit(set144[0].matrix)\\nset144_0_pca = pca.transform(set144[0].matrix)\\nset144_0_pca_img = set144[0].to_matched_img(set144_0_pca)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022fig = plt.figure(figsize=(8, 8))\\nplt.imshow(set144_0_pca_img[:,:,0], cmap=\u0027BuGn\u0027)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022fig = plt.figure(figsize=(8, 8))\\nplt.imshow(set144_0_pca_img[:,:,1], cmap=\u0027BuGn\u0027)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022fig = plt.figure(figsize=(8, 8))\\nplt.imshow(set144_0_pca_img[:,:,2], cmap=\u0027BuGn\u0027)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022# GLCM Textures\\n\\nProcessing time can be pretty brutal so we subset the image. We\u0027ll create texture images so\\nwe can characterize each pixel by the texture of its neighborhood.\\n\\nGLCM is inherently anisotropic but can be averaged so as to be rotation invariant. For more on GLCM, see [the tutorial](http://www.fp.ucalgary.ca/mhallbey/tutorial.htm).\\n\\nA good article on use in remote sensing is [here](http://ieeexplore.ieee.org/xpl/login.jsp?tp=\u0026arnumber=4660321\u0026url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D4660321):\\n\\nPesaresi, M., Gerhardinger, A., \u0026 Kayitakire, F. (2008). A robust built-up area presence index by anisotropic rotation-invariant textural measure. Selected Topics in Applied Earth Observations and Remote Sensing, IEEE Journal of, 1(3), 180-192.\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022sub = set144[0].image[:150,:150,2]\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022def glcm_image(img, measure=\\\u0022dissimilarity\\\u0022):\\n    \\\u0022\\\u0022\\\u0022TODO: allow different window sizes by parameterizing 3, 4. Also should\\n    parameterize direction vector [1] [0]\\\u0022\\\u0022\\\u0022\\n    texture = np.zeros_like(sub)\\n\\n    # quadratic looping in python w/o vectorized routine, yuck!\\n    for i in range(img.shape[0] ):  \\n        for j in range(sub.shape[1] ):  \\n          \\n            # don\u0027t calculate at edges\\n            if (i \u003c 3) or \\\\\\n               (i \u003e (img.shape[0])) or \\\\\\n               (j \u003c 3) or \\\\\\n               (j \u003e (img.shape[0] - 4)):          \\n                continue  \\n        \\n            # calculate glcm matrix for 7 x 7 window, use dissimilarity (can swap in\\n            # contrast, etc.)\\n            glcm_window = img[i-3: i+4, j-3 : j+4]  \\n            glcm = greycomatrix(glcm_window, [1], [0],  symmetric = True, normed = True )   \\n            texture[i,j] = greycoprops(glcm, measure)  \\n    return texture\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022dissimilarity = glcm_image(sub, \\\u0022dissimilarity\\\u0022)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022fig = plt.figure(figsize=(8, 8))\\nplt.subplot(1,2,1)\\nplt.imshow(dissimilarity, cmap=\\\u0022bone\\\u0022)\\nplt.subplot(1,2,2)\\nplt.imshow(sub, cmap=\\\u0022bone\\\u0022)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022# HSV Transform\\n\\nSince this contest is about time series ordering, I think it\u0027s possible there may be useful\\ninformation in a transform to HSV color space. HSV is useful for identifying shadows and illumination, as well\\nas giving us a means to identify similar objects that are distinct by color between scenes (hue), \\nthough there\u0027s no guarantee the hue will be stable.\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022from skimage import color\\n\\nhsv = color.rgb2hsv(set144[0].image)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022fig = plt.figure(figsize=(8, 8))\\nplt.subplot(2,2,1)\\nplt.imshow(set144[0].image, cmap=\\\u0022bone\\\u0022)\\nplt.subplot(2,2,2)\\nplt.imshow(hsv[:,:,0], cmap=\\\u0022bone\\\u0022)\\nplt.subplot(2,2,3)\\nplt.imshow(hsv[:,:,1], cmap=\u0027bone\u0027)\\nplt.subplot(2,2,4)\\nplt.imshow(hsv[:,:,2], cmap=\u0027bone\u0027)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022fig = plt.figure(figsize=(8, 8))\\nplt.subplot(2,2,1)\\nplt.imshow(set144[0].image[:200,:200,:])\\nplt.subplot(2,2,2)\\nplt.imshow(hsv[:200,:200,0], cmap=\\\u0022PuBuGn\\\u0022)\\nplt.subplot(2,2,3)\\nplt.imshow(hsv[:200,:200,1], cmap=\u0027bone\u0027)\\nplt.subplot(2,2,4)\\nplt.imshow(hsv[:200,:200,2], cmap=\u0027bone\u0027)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022fig = plt.figure(figsize=(8, 6))\\nplt.imshow(hsv[200:500,200:500,0], cmap=\u0027bone\u0027)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022hsvmsi = MSImage(hsv)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022# Shadow Detection\\n\\nWe can apply a threshold to the V band now to find dark areas that are probably thresholds. Let\u0027s\\nlook at the distribution of all values then work interactively to find a good filter value.\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022import seaborn as sns\\nsns.distplot(hsvmsi.matrix[:,0], bins=12)\\nsns.distplot(hsvmsi.matrix[:,1], bins=12)\\nsns.distplot(hsvmsi.matrix[:,2], bins=12)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022plt.imshow(hsvmsi.image[:,:,2] \u003c 0.4, cmap=\\\u0022plasma\\\u0022)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022fig = plt.figure(figsize=(8, 8))\\nplt.subplot(1,2,1)\\nplt.imshow(set144[0].image[:250,:250,:])\\nplt.subplot(1,2,2)\\nplt.imshow(hsvmsi.image[:250,:250,2] \u003c 0.4, cmap=\\\u0022plasma\\\u0022)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022fig = plt.figure(figsize=(8, 8))\\nimg2 = plt.imshow(set144[0].image[:250,:250,:], interpolation=\u0027nearest\u0027)\\nimg3 = plt.imshow(hsvmsi.image[:250,:250,2] \u003c 0.4, cmap=\u0027binary_r\u0027, alpha=0.4)\\nplt.show()\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022Could we glean something useful about sun position from shadow orientation if we could accurately\\nreference the image?\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022# Image Registration\\n\\nThis is an earlier form the library found [here](https://github.com/matejak/imreg_dft).\\n\\nBSD family license, reproduced below with copyright so I can utilize similar functions here where\\nimport isn\u0027t available.\\n\\nThis version can be found [here](http://www.lfd.uci.edu/~gohlke/code/imreg.py.html).\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022# -*- coding: utf-8 -*-\\n# imreg.py\\n\\n# Copyright (c) 2011-2014, Christoph Gohlke\\n# Copyright (c) 2011-2014, The Regents of the University of California\\n# Produced at the Laboratory for Fluorescence Dynamics\\n# All rights reserved.\\n#\\n# Redistribution and use in source and binary forms, with or without\\n# modification, are permitted provided that the following conditions are met:\\n#\\n# * Redistributions of source code must retain the above copyright\\n#   notice, this list of conditions and the following disclaimer.\\n# * Redistributions in binary form must reproduce the above copyright\\n#   notice, this list of conditions and the following disclaimer in the\\n#   documentation and/or other materials provided with the distribution.\\n# * Neither the name of the copyright holders nor the names of any\\n#   contributors may be used to endorse or promote products derived\\n#   from this software without specific prior written permission.\\n#\\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \\\u0022AS IS\\\u0022\\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\\n# POSSIBILITY OF SUCH DAMAGE.\\n\\n\\\u0022\\\u0022\\\u0022FFT based image registration.\\n\\nImplements an FFT-based technique for translation, rotation and scale-invariant\\nimage registration [1].\\n\\n:Author:\\n  `Christoph Gohlke \u003chttp://www.lfd.uci.edu/~gohlke/\u003e`_\\n\\n:Organization:\\n  Laboratory for Fluorescence Dynamics, University of California, Irvine\\n\\n:Version: 2013.01.18\\n\\nRequirements\\n------------\\n* `CPython 2.7 or 3.3 \u003chttp://www.python.org\u003e`_\\n* `Numpy 1.7 \u003chttp://www.numpy.org\u003e`_\\n* `Scipy 0.12 \u003chttp://www.scipy.org\u003e`_\\n* `Matplotlib 1.2 \u003chttp://www.matplotlib.org\u003e`_  (optional for plotting)\\n\\nNotes\\n-----\\nThe API and algorithms are not stable yet and are expected to change between\\nrevisions.\\n\\nReferences\\n----------\\n(1) An FFT-based technique for translation, rotation and scale-invariant\\n    image registration. BS Reddy, BN Chatterji.\\n    IEEE Transactions on Image Processing, 5, 1266-1271, 1996\\n(2) An IDL/ENVI implementation of the FFT-based algorithm for automatic\\n    image registration. H Xiea, N Hicksa, GR Kellera, H Huangb, V Kreinovich.\\n    Computers \u0026 Geosciences, 29, 1045-1055, 2003.\\n(3) Image Registration Using Adaptive Polar Transform. R Matungka, YF Zheng,\\n    RL Ewing. IEEE Transactions on Image Processing, 18(10), 2009.\\n\\nExamples\\n--------\\n\u003e\u003e\u003e im0 = imread(\u0027t400\u0027)\\n\u003e\u003e\u003e im1 = imread(\u0027Tr19s1.3\u0027)\\n\u003e\u003e\u003e im2, scale, angle, (t0, t1) = similarity(im0, im1)\\n\u003e\u003e\u003e imshow(im0, im1, im2)\\n\\n\u003e\u003e\u003e im0 = imread(\u0027t350380ori\u0027)\\n\u003e\u003e\u003e im1 = imread(\u0027t350380shf\u0027)\\n\u003e\u003e\u003e t0, t1 = translation(im0, im1)\\n\\n\\\u0022\\\u0022\\\u0022\\n\\nfrom __future__ import division, print_function\\n\\nimport math\\n\\nimport numpy\\nfrom numpy.fft import fft2, ifft2, fftshift\\n\\ntry:\\n    import scipy.ndimage.interpolation as ndii\\nexcept ImportError:\\n    import ndimage.interpolation as ndii\\n\\n__version__ = \u00272013.01.18\u0027\\n__docformat__ = \u0027restructuredtext en\u0027\\n__all__ = [\u0027translation\u0027, \u0027similarity\u0027]\\n\\n\\ndef translation(im0, im1):\\n    \\\u0022\\\u0022\\\u0022Return translation vector to register images.\\\u0022\\\u0022\\\u0022\\n    shape = im0.shape\\n    f0 = fft2(im0)\\n    f1 = fft2(im1)\\n    ir = abs(ifft2((f0 * f1.conjugate()) / (abs(f0) * abs(f1))))\\n    t0, t1 = numpy.unravel_index(numpy.argmax(ir), shape)\\n    if t0 \u003e shape[0] // 2:\\n        t0 -= shape[0]\\n    if t1 \u003e shape[1] // 2:\\n        t1 -= shape[1]\\n    return [t0, t1]\\n\\n\\ndef similarity(im0, im1):\\n    \\\u0022\\\u0022\\\u0022Return similarity transformed image im1 and transformation parameters.\\n\\n    Transformation parameters are: isotropic scale factor, rotation angle (in\\n    degrees), and translation vector.\\n\\n    A similarity transformation is an affine transformation with isotropic\\n    scale and without shear.\\n\\n    Limitations:\\n    Image shapes must be equal and square.\\n    All image areas must have same scale, rotation, and shift.\\n    Scale change must be less than 1.8.\\n    No subpixel precision.\\n\\n    \\\u0022\\\u0022\\\u0022\\n    if im0.shape != im1.shape:\\n        raise ValueError(\\\u0022Images must have same shapes.\\\u0022)\\n    elif len(im0.shape) != 2:\\n        raise ValueError(\\\u0022Images must be 2 dimensional.\\\u0022)\\n\\n    f0 = fftshift(abs(fft2(im0)))\\n    f1 = fftshift(abs(fft2(im1)))\\n\\n    h = highpass(f0.shape)\\n    f0 *= h\\n    f1 *= h\\n    del h\\n\\n    f0, log_base = logpolar(f0)\\n    f1, log_base = logpolar(f1)\\n\\n    f0 = fft2(f0)\\n    f1 = fft2(f1)\\n    r0 = abs(f0) * abs(f1)\\n    ir = abs(ifft2((f0 * f1.conjugate()) / r0))\\n    i0, i1 = numpy.unravel_index(numpy.argmax(ir), ir.shape)\\n    angle = 180.0 * i0 / ir.shape[0]\\n    scale = log_base ** i1\\n\\n    if scale \u003e 1.8:\\n        ir = abs(ifft2((f1 * f0.conjugate()) / r0))\\n        i0, i1 = numpy.unravel_index(numpy.argmax(ir), ir.shape)\\n        angle = -180.0 * i0 / ir.shape[0]\\n        scale = 1.0 / (log_base ** i1)\\n        if scale \u003e 1.8:\\n            raise ValueError(\\\u0022Images are not compatible. Scale change \u003e 1.8\\\u0022)\\n\\n    if angle \u003c -90.0:\\n        angle += 180.0\\n    elif angle \u003e 90.0:\\n        angle -= 180.0\\n\\n    im2 = ndii.zoom(im1, 1.0/scale)\\n    im2 = ndii.rotate(im2, angle)\\n\\n    if im2.shape \u003c im0.shape:\\n        t = numpy.zeros_like(im0)\\n        t[:im2.shape[0], :im2.shape[1]] = im2\\n        im2 = t\\n    elif im2.shape \u003e im0.shape:\\n        im2 = im2[:im0.shape[0], :im0.shape[1]]\\n\\n    f0 = fft2(im0)\\n    f1 = fft2(im2)\\n    ir = abs(ifft2((f0 * f1.conjugate()) / (abs(f0) * abs(f1))))\\n    t0, t1 = numpy.unravel_index(numpy.argmax(ir), ir.shape)\\n\\n    if t0 \u003e f0.shape[0] // 2:\\n        t0 -= f0.shape[0]\\n    if t1 \u003e f0.shape[1] // 2:\\n        t1 -= f0.shape[1]\\n\\n    im2 = ndii.shift(im2, [t0, t1])\\n\\n    # correct parameters for ndimage\u0027s internal processing\\n    if angle \u003e 0.0:\\n        d = int((int(im1.shape[1] / scale) * math.sin(math.radians(angle))))\\n        t0, t1 = t1, d+t0\\n    elif angle \u003c 0.0:\\n        d = int((int(im1.shape[0] / scale) * math.sin(math.radians(angle))))\\n        t0, t1 = d+t1, d+t0\\n    scale = (im1.shape[1] - 1) / (int(im1.shape[1] / scale) - 1)\\n\\n    return im2, scale, angle, [-t0, -t1]\\n\\n\\ndef similarity_matrix(scale, angle, vector):\\n    \\\u0022\\\u0022\\\u0022Return homogeneous transformation matrix from similarity parameters.\\n\\n    Transformation parameters are: isotropic scale factor, rotation angle (in\\n    degrees), and translation vector (of size 2).\\n\\n    The order of transformations is: scale, rotate, translate.\\n\\n    \\\u0022\\\u0022\\\u0022\\n    S = numpy.diag([scale, scale, 1.0])\\n    R = numpy.identity(3)\\n    angle = math.radians(angle)\\n    R[0, 0] = math.cos(angle)\\n    R[1, 1] = math.cos(angle)\\n    R[0, 1] = -math.sin(angle)\\n    R[1, 0] = math.sin(angle)\\n    T = numpy.identity(3)\\n    T[:2, 2] = vector\\n    return numpy.dot(T, numpy.dot(R, S))\\n\\n\\ndef logpolar(image, angles=None, radii=None):\\n    \\\u0022\\\u0022\\\u0022Return log-polar transformed image and log base.\\\u0022\\\u0022\\\u0022\\n    shape = image.shape\\n    center = shape[0] / 2, shape[1] / 2\\n    if angles is None:\\n        angles = shape[0]\\n    if radii is None:\\n        radii = shape[1]\\n    theta = numpy.empty((angles, radii), dtype=numpy.float64)\\n    theta.T[:] = -numpy.linspace(0, numpy.pi, angles, endpoint=False)\\n    #d = radii\\n    d = numpy.hypot(shape[0]-center[0], shape[1]-center[1])\\n    log_base = 10.0 ** (math.log10(d) / (radii))\\n    radius = numpy.empty_like(theta)\\n    radius[:] = numpy.power(log_base, numpy.arange(radii,\\n                                                   dtype=numpy.float64)) - 1.0\\n    x = radius * numpy.sin(theta) + center[0]\\n    y = radius * numpy.cos(theta) + center[1]\\n    output = numpy.empty_like(x)\\n    ndii.map_coordinates(image, [x, y], output=output)\\n    return output, log_base\\n\\n\\ndef highpass(shape):\\n    \\\u0022\\\u0022\\\u0022Return highpass filter to be multiplied with fourier transform.\\\u0022\\\u0022\\\u0022\\n    x = numpy.outer(\\n        numpy.cos(numpy.linspace(-math.pi/2., math.pi/2., shape[0])),\\n        numpy.cos(numpy.linspace(-math.pi/2., math.pi/2., shape[1])))\\n    return (1.0 - x) * (2.0 - x)\\n\\n\\ndef imread(fname, norm=True):\\n    \\\u0022\\\u0022\\\u0022Return image data from img\u0026hdr uint8 files.\\\u0022\\\u0022\\\u0022\\n    with open(fname+\u0027.hdr\u0027, \u0027r\u0027) as fh:\\n        hdr = fh.readlines()\\n    img = numpy.fromfile(fname+\u0027.img\u0027, numpy.uint8, -1)\\n    img.shape = int(hdr[4].split()[-1]), int(hdr[3].split()[-1])\\n    if norm:\\n        img = img.astype(numpy.float64)\\n        img /= 255.0\\n    return img\\n\\n\\ndef imshow(im0, im1, im2, im3=None, cmap=None, **kwargs):\\n    \\\u0022\\\u0022\\\u0022Plot images using matplotlib.\\\u0022\\\u0022\\\u0022\\n    from matplotlib import pyplot\\n    if cmap is None:\\n        cmap = \u0027coolwarm\u0027\\n    if im3 is None:\\n        im3 = abs(im2 - im0)\\n    pyplot.subplot(221)\\n    pyplot.imshow(im0, cmap, **kwargs)\\n    pyplot.subplot(222)\\n    pyplot.imshow(im1, cmap, **kwargs)\\n    pyplot.subplot(223)\\n    pyplot.imshow(im3, cmap, **kwargs)\\n    pyplot.subplot(224)\\n    pyplot.imshow(im2, cmap, **kwargs)\\n    pyplot.show()\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022We read in two files from the original set and compare them.\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022img1 = plt.imread(set175[1])\\nimg2 = plt.imread(set175[3])\\nplt.figure(figsize=(8,10))\\nplt.subplot(121)\\nplt.imshow(img1)\\nplt.subplot(122)\\nplt.imshow(img2)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022Now transform `img2` red band to align with `img1` red band.\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022img3, scale, angle, (t0, t1) = similarity(img1[:,:,2], img2[:,:,2])\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022# Viewing registration\\n\\nWe should be looking at:\\n    \\n* UL: template for transformation\\n* UR: image that was transformed\\n* LL: diff after transformation\\n* LR: transformed image\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022imshow(img1[:,:,2], img2[:,:,2], img3)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022# just diff\\nplt.imshow(img3 - img1[:,:,2])\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022# well, was working in a standalone private notebook, need to figure out what\u0027s off.\\n# it\u0027s fairly obviously offset/translation/shift is wrong somehow.\\nplt.imshow(img3)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022# NN with Downscaled Red Band Image\\n\\nI\u0027m exploring the best way to feed images into a siamese net like the one described [here](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zagoruyko_Learning_to_Compare_2015_CVPR_paper.pdf) and [here](http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf).\\n\\nIdeally you\u0027d want to build the DNN to function this way:\\n\\n* Feed in two images as in diagram in paper [1]:\\n\\\u0022A central-surround two-stream network that uses a\\nsiamese-type architecture to process each stream\\\u0022\\n* Train the DNN to learn a simple comparison function: 1 if image is before, 0 if false (no simultaneously captured images so no equality).\\n* Once you have this comparator, then you can apply a sorting alrogithm to sort each dataset.\\n\\nThere are lots of details to be worked out here, still -- mainly trying to select overlapping image patches.\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022from skimage.transform import downscale_local_mean\\n\\nimg1 = plt.imread(set175[1])[:,:,2]\\nimg2 = plt.imread(set175[3])[:,:,2]\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022np.shape(img1), np.shape(img2)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022img1ds = downscale_local_mean(img1, (10, 10))\\nimg2ds = downscale_local_mean(img2, (10, 10))\\nplt.figure(figsize=(8,10))\\nplt.subplot(121)\\nplt.imshow(img1ds[:225,:300])\\nplt.subplot(122)\\nplt.imshow(img2ds[:225,:300])\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022For prototyping my goal is to get a bunch of downscaled images so that I can feed them in to test\\nplausibility of basic NN architecture decisions.\\n\\nDims are subsampled and hard-coded for now so we don\u0027t have wrong dimensions showing up here and there.\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022def read_and_downscale(f):\\n    img = plt.imread(f)\\n    ds = downscale_local_mean(img[:,:,2], (10, 10))\\n    return ds[:225, :300]\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022set79 = [read_and_downscale(\\\u0022../input/train_sm/set79_\\\u0022 + str(n) + \\\u0022.jpeg\\\u0022) for n in range(1, 6)]\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022def read_ds_set(setn):\\n    match_pre = \\\u0022../input/train_sm/set\\\u0022 + str(setn) + \\\u0022_\\\u0022\\n    return [read_and_downscale(match_pre + str(n) + \\\u0022.jpeg\\\u0022) for n in range(1, 6)]\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022set79 = read_ds_set(79)\\nset285 = read_ds_set(285)\\nset35 = read_ds_set(35)\\nset175 = read_ds_set(175)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022plt.imshow(set79[0])\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022plt.imshow(set285[2])\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022plt.imshow(set35[3])\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022# pairing function\\n\\nWe want to traverse multiple lists of images and construct all naive before/after pairs.\\n\\n* Note there are actually more before/after pairs in this space, but this blows up quickly, at\\n  least for little Kaggle notebooks.\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022def get_pairs(imgls):\\n    pairl = []\\n    for imgl in imgls:\\n        pairl += [(a,b) for a,b in zip(imgl[:-1], imgl[1:])]\\n    return pairl\\n            \\npaired = get_pairs([set35, set79, set285, set175])\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022# Let\u0027s just start with a dorky example.\\nrev_pairs = [(imgb, imga) for imga, imgb in paired]\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022img_a, img_b = rev_pairs[0]\\nconcat_img = np.vstack((img_a, img_b))\\nprint(np.shape(concat_img))\\nplt.imshow(concat_img)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022import random\\n\\ndef concatter(imgpairs):\\n    for a, b in imgpairs:\\n        yield np.vstack((a, b))\\n\\nconcats = [cimg for cimg in concatter(paired + rev_pairs)]\\nrandom.shuffle(concats)\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022# Oops!\\n\\nShuffled before supplying labels elsewhere about whether it\u0027s forward or backward comparison.\\n\\nGoing to have to leave this for tonight anyways.\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022plt.figure(figsize=(10,15))\\nplt.subplot(321)\\nplt.imshow(concats[0])\\nplt.subplot(322)\\nplt.imshow(concats[1])\\nplt.subplot(323)\\nplt.imshow(concats[2])\\nplt.subplot(324)\\nplt.imshow(concats[3])\\nplt.subplot(325)\\nplt.imshow(concats[4])\\nplt.subplot(326)\\nplt.imshow(concats[5])\\nplt.show()\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022code\u0022,\n  \u0022execution_count\u0022: null,\n  \u0022metadata\u0022: {\n   \u0022collapsed\u0022: false\n  },\n  \u0022outputs\u0022: [],\n  \u0022source\u0022: \u0022# Need to simplify or build up next block from convnet template.\u0022\n },\n {\n  \u0022cell_type\u0022: \u0022markdown\u0022,\n  \u0022metadata\u0022: {},\n  \u0022source\u0022: \u0022```python\\nfrom keras.models import Sequential\\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D\\n\\n# Need to add dropouts\\nmodel = Sequential()\\n\\n# conv layer 1\\nmodel.add(Convolution2D(50,1,2,2))\\nmodel.add(Activation(\u0027relu\u0027))\\n\\n# conv layer 2\\nmodel.add(Convolution2D(50, 32, 2, 2))\\nmodel.add(Activation(\u0027relu\u0027)) \\nmodel.add(MaxPooling2D(poolsize=(2,2)))\\n\\n# conv layer 3\\nmodel.add(Convolution2D(50, 50, 2, 2))\\nmodel.add(Activation(\u0027relu\u0027))\\nmodel.add(MaxPooling2D(poolsize=(2,2)))\\n\\n# feed to fully connected \\nmodel.add(Flatten())\\n\\n# first fully connected\\nmodel.add(Dense(1000, 128, init=\u0027glorot_uniform\u0027))\\nmodel.add(Activation(\u0027relu\u0027))\\nmodel.add(Dropout(0.25))\\n\\n# next fully connected\\nmodel.add(Dense(128, 64, init=\u0027glorot_uniform\u0027))\\nmodel.add(Activation(\u0027relu\u0027))\\nmodel.add(Dropout(0.25))\\n\\n# last fully connected which outputs comparison result\\nmodel.add(Dense(64, 1, init=\u0027glorot_uniform\u0027))\\nmodel.add(Activation(\u0027sigmoid\u0027))\\n\\n# compile model\\n# model.compile(loss=\u0027binary_crossentropy\u0027, optimizer=\\\u0022rmsprop\\\u0022)\\n```\u0022\n }\n],\u0022metadata\u0022:{\u0022kernelspec\u0022:{\u0022display_name\u0022:\u0022Python 3\u0022,\u0022language\u0022:\u0022python\u0022,\u0022name\u0022:\u0022python3\u0022}}, \u0022nbformat\u0022: 4, \u0022nbformat_minor\u0022: 0}","languageName":"Python","htmlOutputFileUrl":"https://www.kaggle.io/svf/232673/c8a9b13d2bad2b7d8ceb8af8b0e9db13/__results__.html","outputFiles":[{"fileName":"results.zip","fileUrl":"https://www.kaggle.io/svf/232673/c8a9b13d2bad2b7d8ceb8af8b0e9db13/results.zip","fileType":".zip","fileLinesUrlTemplate":"/svcj/232673/c8a9b13d2bad2b7d8ceb8af8b0e9db13/101/results.zip","linesToSend":101,"csvHeaderValidationErrors":"","size":7227588}],"downloadAllFilesUrl":"/scripts/svzip/232673","runInfo":{"isValidStatus":true,"failureMessage":"","exitCode":0,"succeeded":true,"dockerImageName":"kaggle/python","dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"1eef4438a447dc722ce2ff252434522172ebed7b853444f67d3461f122dc9de6","timeoutExceeded":false,"runTimeSeconds":370.078255653381,"usedAllSpace":false,"queuedSeconds":0.13,"outputSizeBytes":14852847},"versionHistory":[{"id":232673,"title":"Exploratory Image Analysis","isForkParent":false,"url":"/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/run/232673","versionNumber":12,"lastRunTime":"2016-05-11T22:05:56.787Z","status":"complete","runInfo":{"isValidStatus":true,"failureMessage":"","exitCode":0,"succeeded":true,"dockerImageName":"kaggle/python","dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"1eef4438a447dc722ce2ff252434522172ebed7b853444f67d3461f122dc9de6","timeoutExceeded":false,"runTimeSeconds":370.078255653381,"usedAllSpace":false},"outputFilesTotalSizeBytes":14852847,"languageName":"Python","isNotebook":true,"linesChangedFromPrevious":0,"linesInsertedFromPrevious":0,"linesDeletedFromPrevious":0},{"id":232017,"title":"Exploratory Image Analysis","isForkParent":false,"url":"/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/run/232017","versionNumber":11,"lastRunTime":"2016-05-11T04:58:46.07Z","status":"complete","runInfo":{"isValidStatus":true,"failureMessage":"","exitCode":0,"succeeded":true,"dockerImageName":"kaggle/python","dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"1eef4438a447dc722ce2ff252434522172ebed7b853444f67d3461f122dc9de6","timeoutExceeded":false,"runTimeSeconds":423.218817472458,"usedAllSpace":false},"outputFilesTotalSizeBytes":13732848,"languageName":"Python","isNotebook":true,"linesChangedFromPrevious":1,"linesInsertedFromPrevious":1,"linesDeletedFromPrevious":1},{"id":228313,"title":"Exploratory Image Analysis","isForkParent":false,"url":"/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/run/228313","versionNumber":10,"lastRunTime":"2016-05-05T21:31:32.887Z","status":"complete","runInfo":{"isValidStatus":true,"failureMessage":"","exitCode":0,"succeeded":true,"dockerImageName":"kaggle/python","dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"1eef4438a447dc722ce2ff252434522172ebed7b853444f67d3461f122dc9de6","timeoutExceeded":false,"runTimeSeconds":373.073547124863,"usedAllSpace":false},"outputFilesTotalSizeBytes":12358532,"languageName":"Python","isNotebook":true,"linesChangedFromPrevious":1,"linesInsertedFromPrevious":5,"linesDeletedFromPrevious":0},{"id":227665,"title":"Exploratory Image Analysis","isForkParent":false,"url":"/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/run/227665","versionNumber":9,"lastRunTime":"2016-05-05T03:16:48.167Z","status":"complete","runInfo":{"isValidStatus":true,"failureMessage":"","exitCode":0,"succeeded":true,"dockerImageName":"kaggle/python","dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"sha256:d20487605aea09d96040519ce1f8e38f4801e518c03467c9365053d3fb4fbb1b","timeoutExceeded":false,"runTimeSeconds":369.857989549637,"usedAllSpace":false},"outputFilesTotalSizeBytes":10284277,"languageName":"Python","isNotebook":true,"linesChangedFromPrevious":0,"linesInsertedFromPrevious":0,"linesDeletedFromPrevious":1},{"id":224451,"title":"Exploratory Image Analysis","isForkParent":false,"url":"/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/run/224451","versionNumber":8,"lastRunTime":"2016-05-01T20:35:48.637Z","status":"complete","runInfo":{"isValidStatus":true,"failureMessage":"","exitCode":0,"succeeded":true,"dockerImageName":"kaggle/python","dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"sha256:d20487605aea09d96040519ce1f8e38f4801e518c03467c9365053d3fb4fbb1b","timeoutExceeded":false,"runTimeSeconds":845.539191484451,"usedAllSpace":false},"outputFilesTotalSizeBytes":6573648,"languageName":"Python","isNotebook":true,"linesChangedFromPrevious":0,"linesInsertedFromPrevious":3,"linesDeletedFromPrevious":1},{"id":223587,"title":"Exploratory Image Analysis","isForkParent":false,"url":"/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/run/223587","versionNumber":7,"lastRunTime":"2016-04-30T21:51:40.117Z","status":"complete","runInfo":{"isValidStatus":true,"failureMessage":"","exitCode":0,"succeeded":true,"dockerImageName":"kaggle/python","dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"1eef4438a447dc722ce2ff252434522172ebed7b853444f67d3461f122dc9de6","timeoutExceeded":false,"runTimeSeconds":524.093641042709,"usedAllSpace":false},"outputFilesTotalSizeBytes":3343898,"languageName":"Python","isNotebook":true,"linesChangedFromPrevious":0,"linesInsertedFromPrevious":0,"linesDeletedFromPrevious":0},{"id":222995,"title":"Exploratory Image Analysis","isForkParent":false,"url":"/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/run/222995","versionNumber":6,"lastRunTime":"2016-04-30T05:02:29.757Z","status":"complete","runInfo":{"isValidStatus":true,"failureMessage":"","exitCode":0,"succeeded":true,"dockerImageName":"kaggle/python","dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"sha256:d20487605aea09d96040519ce1f8e38f4801e518c03467c9365053d3fb4fbb1b","timeoutExceeded":false,"runTimeSeconds":240.539965629578,"usedAllSpace":false},"outputFilesTotalSizeBytes":2323942,"languageName":"Python","isNotebook":true,"linesChangedFromPrevious":0,"linesInsertedFromPrevious":0,"linesDeletedFromPrevious":0},{"id":222990,"title":"Exploratory Image Analysis","isForkParent":false,"url":"/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/run/222990","versionNumber":5,"lastRunTime":"2016-04-30T04:55:58.873Z","status":"error","runInfo":{"isValidStatus":true,"failureMessage":"The script was killed, likely for trying to exceed the memory limit of 8192M.","exitCode":137,"succeeded":false,"dockerImageName":"kaggle/python","dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"1eef4438a447dc722ce2ff252434522172ebed7b853444f67d3461f122dc9de6","timeoutExceeded":false,"runTimeSeconds":1200.40210604668,"usedAllSpace":false},"outputFilesTotalSizeBytes":8096,"languageName":"Python","isNotebook":true,"linesChangedFromPrevious":1,"linesInsertedFromPrevious":2,"linesDeletedFromPrevious":0},{"id":222986,"title":"Exploratory Image Analysis","isForkParent":false,"url":"/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/run/222986","versionNumber":4,"lastRunTime":"2016-04-30T04:44:44.66Z","status":"complete","runInfo":{"isValidStatus":true,"failureMessage":"","exitCode":0,"succeeded":true,"dockerImageName":"kaggle/python","dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"1eef4438a447dc722ce2ff252434522172ebed7b853444f67d3461f122dc9de6","timeoutExceeded":false,"runTimeSeconds":245.411792516708,"usedAllSpace":false},"outputFilesTotalSizeBytes":2329833,"languageName":"Python","isNotebook":true,"linesChangedFromPrevious":1,"linesInsertedFromPrevious":1,"linesDeletedFromPrevious":0},{"id":222977,"title":"Exploratory Image Analysis","isForkParent":false,"url":"/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/run/222977","versionNumber":3,"lastRunTime":"2016-04-30T04:23:55.6Z","status":"complete","runInfo":{"isValidStatus":true,"failureMessage":"","exitCode":0,"succeeded":true,"dockerImageName":"kaggle/python","dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"sha256:d20487605aea09d96040519ce1f8e38f4801e518c03467c9365053d3fb4fbb1b","timeoutExceeded":false,"runTimeSeconds":360.678575754166,"usedAllSpace":false},"outputFilesTotalSizeBytes":2342328,"languageName":"Python","isNotebook":true,"linesChangedFromPrevious":0,"linesInsertedFromPrevious":268,"linesDeletedFromPrevious":0},{"id":222975,"title":"Exploratory Image Analysis","isForkParent":false,"url":"/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/run/222975","versionNumber":2,"lastRunTime":"2016-04-30T04:21:50.743Z","status":"error","runInfo":{"isValidStatus":true,"failureMessage":"The script was killed for running longer than 1200 seconds.","exitCode":137,"succeeded":false,"dockerImageName":"kaggle/python","dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"sha256:d20487605aea09d96040519ce1f8e38f4801e518c03467c9365053d3fb4fbb1b","timeoutExceeded":true,"runTimeSeconds":1200.4548535347,"usedAllSpace":false},"outputFilesTotalSizeBytes":8091,"languageName":"Python","isNotebook":true,"linesChangedFromPrevious":0,"linesInsertedFromPrevious":0,"linesDeletedFromPrevious":0},{"id":222936,"title":"Exploratory Image Analysis","isForkParent":false,"url":"/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis/run/222936","versionNumber":1,"lastRunTime":"2016-04-30T03:07:53.793Z","status":"complete","runInfo":{"isValidStatus":true,"failureMessage":"","exitCode":0,"succeeded":true,"dockerImageName":"kaggle/python","dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"1eef4438a447dc722ce2ff252434522172ebed7b853444f67d3461f122dc9de6","timeoutExceeded":false,"runTimeSeconds":13.824654340744,"usedAllSpace":false},"outputFilesTotalSizeBytes":223430,"languageName":"Python","isNotebook":true,"linesChangedFromPrevious":0,"linesInsertedFromPrevious":0,"linesDeletedFromPrevious":0}],"sanitizeHtml":false,"initialScriptVersionId":232673,"initialTab":null,"baseUrl":"/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis","toggleScriptIsHiddenUrl":"/scripts/58886/hidden/toggle","scriptIsHidden":false,"isAdmin":false,"toggleScriptLanguageTemplateUrl":"/scripts/58886/template/toggle","scriptIsLanguageTemplate":false,"rerunScriptUrl":"/scripts/rerun?id=232673","forksBaseUrl":"/scripts/forks/58886/0","hasForks":true,"newForkUrl":"/c/draper-satellite-image-chronology/scripts/fork/232673","isNotebook":true,"isrMarkdown":false,"scriptId":58886,"scriptVersionId":232673,"getCommentsUrl":"/scripts/comments","postCommentUrl":"/scripts/comments/ajax","scriptVersionHasError":false,"userCanEditScript":false,"showSubmitToCompButtons":true,"submitToCompUrl":"/competitions/scripts/submit","competitionName":"Draper Satellite Image Chronology","currentScriptVersionIsBusy":false,"statusCallbackUrl":"/scripts/status?id=232673","forkParentUrl":"","forkParentTitle":null,"forkParentAuthorUrl":null,"forkParentAuthorDisplayName":null,"forkDiffUrl":"/scripts/diff/0/232673","forkDiffLinesChanged":0,"forkDiffLinesDeleted":0,"forkDiffLinesInserted":0,"voteButton":{"totalVotes":83,"hasAlreadyVotedUp":false,"hasAlreadyVotedDown":false,"canUpvote":true,"canDownvote":false,"voteUpUrl":"/scripts/vote?id=58886","voteDownUrl":null,"voters":[{"userId":199350,"displayName":"Frank Rigsby","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/52aa223efcfce9692099f6b13371d416.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/rigger"},{"userId":197832,"displayName":"sebasibarguen","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/d3f606b7df1cbcc2c045dcac203abec4.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/sebasibarguen"},{"userId":611745,"displayName":"cybermac","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/cddbda66cfbe2a2e9458b9f2fdd25b87.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/cybermac"},{"userId":270021,"displayName":"WW44SS","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/3176fb6712eeafe13e2745cb59d1360d.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/ww44ss"},{"userId":248917,"displayName":"Salah Uddin","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/d6aeba7b678835c0599bb7deafc4d9a6.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/salahuddin"},{"userId":234886,"displayName":"true_pk","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/6550e17e7509fd849503b862cd5baebe.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/truepk"},{"userId":191861,"displayName":"Michael Pawlus","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/2dd8eec7f06f0c6ccaa6d42787bc59dd.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/michaelpawlus"},{"userId":630288,"displayName":"WiNDWAY","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/da360f90a0c15b8ac2ccd4bab2dae5ec.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/alvin319"},{"userId":395813,"displayName":"sahanavaradaraj","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/fe409ea046142afe60bc75300297cf79.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/sahanakv"},{"userId":373165,"displayName":"Guilherme Marmerola","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/17e956f990d9267d87323e350a4ec0e6.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/gdmarmerola"},{"userId":374340,"displayName":"Graymalk","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/7b2563e68007c5662d8ed02f88ccb914.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/graymalk"},{"userId":145129,"displayName":"Katerina","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/7562140103e9751aa36d243f3b77470f.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/taneta"},{"userId":591779,"displayName":"Sarvesh","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/d15004dbe3dff0f857e1f586678cc859.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/sarveshsingh8"},{"userId":570681,"displayName":"qqoopp","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/587a9e37973772ce600396bb59394684.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/qqoopp"},{"userId":46400,"displayName":"Sehori","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/c272885c33a08a81fd05339152db3fc7.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/users/46400/sehori"},{"userId":614661,"displayName":"skyfalse","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/1d40617d87e957d67df267c8b80bd4a9.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/skyfalse"},{"userId":611127,"displayName":"azimuth","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/04fec76ac404cd86eb6759585f68efa5.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/azimuth"},{"userId":232976,"displayName":"wittmaan","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/06d44d5aaac497e00d8ccaf8d4edd808.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/wittmaan"},{"userId":510616,"displayName":"Peter","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/39eed8890a5df0e0093c0637f53d4f45.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/patricker"},{"userId":269652,"displayName":"Hedi","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/4ec45863e1e63429c719ace24113269c.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/hh25di"},{"userId":521913,"displayName":"staggerlee","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/3b2b4102c8e737a68157dbd699f282e8.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/brettamdur"},{"userId":472551,"displayName":"BB001","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/8dde56031a6055625451aa16fb27d430.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/blackbird001"},{"userId":161256,"displayName":"Alex Zillion","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/97fd2fc4b24f71f2d585baea53846710.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/ansemwise"},{"userId":172375,"displayName":"Srinivasan Krishnan","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/a902fbd8c8f6a1855f3c36dc5669e86e.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/srikrish"},{"userId":295944,"displayName":"Nimit Pattanasri","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/20970d01a07d36af92cca46b398b5407.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/scannerd"},{"userId":466744,"displayName":"CaoLu","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/b4c901812be6583aa2c08e8e35cbb472.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/data0caolu"},{"userId":474797,"displayName":"syedmehtab","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/de8e590e721e9d11609215a7f3901de1.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/mehtab"},{"userId":200945,"displayName":"nk41","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/d1517cb1e0053303fce0c526ee49f9e9.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/nk41kag"},{"userId":514881,"displayName":"omkardixit","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/6345168cf6b246790526bafc2a71ea35.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/dixitomkar"},{"userId":18463,"displayName":"Mathurin Aché","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/539d2d918ff8bf60646bb25fe2a78844.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/mathurinache"},{"userId":507936,"displayName":"Darren Wilkinson","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/a195d1df52b0dc37a7b1c0ae835e367c.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/wilkinsondi"},{"userId":610222,"displayName":"YangLiu","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/7a62dd776f39eff067e390c695604bc8.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/liuy0095"},{"userId":8376,"displayName":"Matt","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/059714f4428e8ef53b809ee923203531.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/mattvonrohr"},{"userId":400262,"displayName":"Alexandru Papiu","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/88ce774c2533fbcea52e7f37dc2b1a09.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/apapiu"},{"userId":449096,"displayName":"Andrew Khalel","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/01bda9cd087601faebbe2910b013b77e.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/andrewkh"},{"userId":317195,"displayName":"Me","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/98b57775247f02a9defee655687270ce.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/whatsaclassifier"},{"userId":31798,"displayName":"sporeza","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/ba29632287d8d1034eaa7a57e556a2fe.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/sporeza"},{"userId":344688,"displayName":"the1owl","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/2664b23f76b5fc9c3f767e0d3577f561.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/the1owl"},{"userId":606613,"displayName":"Mark Young","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/41211629255b67fcb801bc003c1c770f.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/mymountainb"},{"userId":441694,"displayName":"alex_","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/c6703b19f94a6bfd224d932f4ec4fbcb.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/alexandrearaujo"},{"userId":380246,"displayName":"IkerS","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/2d1f8fc05faf12483ccccc75b12f8064.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/ikersanchez"},{"userId":510980,"displayName":"Rishikesh","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/3284f55d6bf0f8b0fec59cea67e24de0.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/rishikksh20"},{"userId":285810,"displayName":"xyz","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/2daafc4cedf04ada5779dbd527249789.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/zhugds"},{"userId":5940,"displayName":"Charlie Turner","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/b161c2feea49c620bac883d2cfcb0e85.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/cjturner"},{"userId":93684,"displayName":"duyvk","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/edbbf798696737dfebf726266bd7bbe9.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/duyvkvn"},{"userId":216233,"displayName":"Ehsan","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/7881c4b6271f3b501fe6930aa1798907.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/ehsanmkermani"},{"userId":461671,"displayName":"Dmitriy","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/ecd2f36dcf15c071422a71362092b04e.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/caspik"},{"userId":587398,"displayName":"Ali","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/0cabce7360808dc151d3d7c98788bb44.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/aliajouz"},{"userId":580201,"displayName":"mrcsv16","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/937e1899e0fe39b746f066761b9a7aa9.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/datascientist2016"},{"userId":259561,"displayName":"Anna Montoya","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/1d77db438142f247317cf26ace4983fe.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/annavictoria"},{"userId":495305,"displayName":"Megan Risdal","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/00863821b067957d24cbce09334580aa.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/mrisdal"},{"userId":265542,"displayName":"Özgün Genç","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/dcfe839483dd695710d884b4ab664761.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/ozgung"},{"userId":254941,"displayName":"HEMANT KUMAR","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/8a696fe480f53e7e9cee0598b4eb6a56.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/hemantime"},{"userId":606183,"displayName":"MarkJVogel","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/0a30652752b02e639eeb1204298b7fd5.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/markjvogel"},{"userId":271153,"displayName":"Miguel Batista","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/eb874ab37c58b795a69cb88074d9d49a.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/mbatista"},{"userId":66866,"displayName":"itod","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/2a365642c2bc38e0bfba16ade9f70906.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/itodorovic"},{"userId":539933,"displayName":"CliffWeng","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/9b893a2a30b4a8010da5d85d311e3687.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/cliffweng"},{"userId":221112,"displayName":"madcap","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/b6202be3f5426f166f3f60d7f6ad0d2f.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/madcap"},{"userId":337607,"displayName":"Aleksandr Meshkov","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/502bbea5af32970ee7615b5532a38788.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/ameshkov"},{"userId":183041,"displayName":"hamelg","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/00c3054559e6baf023651776dc37ae76.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/hamelg"},{"userId":605297,"displayName":"t.metoki","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/57dfd5d2587b2bff3e2c39baf00a9ab6.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/metoki"},{"userId":156509,"displayName":"Paul Stadnikov","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/558e79d085477b2f67bb0b75fdacb68c.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/pawlyk"},{"userId":29289,"displayName":"Wei Wu","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/cf0eeb52c8820c995acf58f923c1e2cb.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/weiwunyc"},{"userId":14756,"displayName":"Chippy","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/530168c56719cdc4abd601aa94848e29.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/nigelcarpenter"},{"userId":589774,"displayName":"B.S.Kishore","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/2e600bd1e37b09dbe0854bdb6446fae5.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/nanobiotechguy"},{"userId":252874,"displayName":"SkyLord","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/8fd940ab1c45c30bc47ab7ac296e66f9.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/skylord"},{"userId":313914,"displayName":"Eryk Walczak","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/c625e28a03d84c56ba0d06fe6b18b997.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/erykwalczak"},{"userId":199877,"displayName":"Ayşe Elvan Gündüz","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/08054d470b13bb5bc1138a1ac0aef10a.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/aelvangunduz"},{"userId":467421,"displayName":"PremanandKumar","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/3a29bfb2d8ce3f86d1ed889237fce97c.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/premanandkumar"},{"userId":31394,"displayName":"Thomas Jungblut","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/fad9dcf9bf2f17d3b61fa583595bb0d6.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/tjungblut"},{"userId":402598,"displayName":"PerryCheng","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/2c35f06946b90277d2e15a448d87e355.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/kitman0804"},{"userId":993,"displayName":"Ben Hamner","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/faeafd58bd079a10dac1e0d15247e808.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/benhamner"},{"userId":3258,"displayName":"William Cukierski","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/3788b116795ee839597f2f238c7b5440.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/wcukierski"},{"userId":207782,"displayName":"vivek yadav","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/dd55687c34ed8906161ecb27bcfe2f48.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/vivekyadav"},{"userId":189635,"displayName":"ramli","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/fa9d1775636fb69f5d87699bfbc41db2.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/ramliprasad"},{"userId":87114,"displayName":"smota","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/96342b7ab1a902d39d1237b81a92554b.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/santiagomota"},{"userId":260312,"displayName":"Mehdi L.","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/4c714f49d1d54bc6f2d22881638511fe.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/lmehdi"},{"userId":340845,"displayName":"sovankrishna","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/c0249d880e8c95293d0656571eee8b45.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/chsovankrishna"},{"userId":600282,"displayName":"openthings","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/2dfbedcf457b30013a025aa88569bb70.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/openthings"},{"userId":82927,"displayName":"iyarmolau","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/7270a62d2b3c4d5af6734c17bfbf4226.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/yarmolau"},{"userId":333784,"displayName":"MelvinDunn","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/10e39c4c39986a8ee12eac29ef17655b.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/melvindunn"},{"userId":411185,"displayName":"twalters","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/e3e0a711f0a8d168681b9085b626f24a.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/mrslimification"},{"userId":329613,"displayName":"Khaled Fayed","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/41123ee26c1901a337b7eda63ef24844.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/khaledfayed"}],"currentUserInfo":{"userId":378178,"displayName":"Abhinav Sharma ","avatarThumbnailUrl":"https://secure.gravatar.com/avatar/f3479e3e7744f6341361fd80ae61adb0.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","profileUrl":"/abhinavs01858"},"showVoters":true,"upVoteIntercomEventName":"upvote_script"},"dateUpdated":"2016-05-11T22:05:56.917Z","userName":"Ben Kamphaus","userUrl":"/bkamphaus","userAvatarImageUrl":"https://secure.gravatar.com/avatar/4a002dced7ef642b40ba52c96689d43f.jpg?r=pg\u0026s=100\u0026d=https%3a%2f%2fkaggle2.blob.core.windows.net%2favatars%2fthumbnails%2fdefault-thumb.png","parentName":"Draper Satellite Image Chronology","parentUrl":"/c/draper-satellite-image-chronology/scripts","scriptLanguage":"Python notebook","totalViews":11951,"title":"Exploratory Image Analysis","titleUrl":"/bkamphaus/draper-satellite-image-chronology/exploratory-image-analysis","thumbnailImageUrl":"https://kaggle2.blob.core.windows.net/competitions/kaggle/5229/logos/thumb76_76.png","menuLinks":[{"href":null,"text":"Notebook","title":"Notebook","tab":"notebook","count":null,"showZeroCountExplicitly":false},{"href":null,"text":"Code","title":"Code","tab":"code","count":null,"showZeroCountExplicitly":false},{"href":null,"text":"Output","title":"Output","tab":"output","count":1,"showZeroCountExplicitly":false},{"href":null,"text":"Comments","title":"Comments","tab":"comments","count":21,"showZeroCountExplicitly":true},{"href":null,"text":"Log","title":"Log","tab":"log","count":null,"showZeroCountExplicitly":false},{"href":null,"text":"Versions","title":"Versions","tab":"versions","count":12,"showZeroCountExplicitly":false},{"href":null,"text":"Forks","title":"Forks","tab":"forks","count":37,"showZeroCountExplicitly":false}],"rightMenuLinks":[],"callToAction":{"href":"/c/draper-satellite-image-chronology/scripts/forknb/232673","text":"Fork Notebook","title":"Fork Notebook","tab":null,"count":null,"showZeroCountExplicitly":false}}), document.getElementById("react-ScriptViewer-1"));
ReactDOM.render(React.createElement(KaggleReactComponents.SiteHeaderContainer, {}), document.getElementById("react-SiteHeaderContainer-2"));

</script><textarea style="letter-spacing:normal;line-height:19.6px;padding-top:10px;padding-bottom:10px;font-family:&quot;Atlas Grotesk&quot;, sans-serif;font-weight:100;font-size:14px;text-rendering:auto;text-transform:none;width:899.566px;text-indent:0px;padding-left:10px;padding-right:10px;border-width:1.11111px;box-sizing:border-box;
  min-height:0 !important;
  max-height:none !important;
  height:0 !important;
  visibility:hidden !important;
  overflow:hidden !important;
  position:absolute !important;
  z-index:-1000 !important;
  top:0 !important;
  right:0 !important
"></textarea>






    <script type="text/javascript">
        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-12629138-1']);
        _gaq.push(['_trackPageview']);
        _gaq.push(['_trackPageLoadTime']);
        _gaq.push(['_setCustomVar', 1, 'usertype', 'registered', 2]);
            _gaq.push(['_setCustomVar', 2, 'userid', '378178', 2]);
        (function () {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();
    
       
        var trackOutboundLink = function(url) {
            ga('send', 'event', 'outbound', 'click', url, {'hitCallback':
                    function () {
                        document.location = url;
                    }
            });
        }
    </script>




<!-- Cheers, RD00155D488586p. -->



</body></html>